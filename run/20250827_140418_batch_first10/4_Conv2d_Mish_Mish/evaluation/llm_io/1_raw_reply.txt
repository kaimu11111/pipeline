import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

// -----------------------------------------------------------------------------
// Numerically–stable Softplus
// -----------------------------------------------------------------------------
template <typename scalar_t>
__device__ __forceinline__ scalar_t softplus(const scalar_t x)
{
    const scalar_t threshold = static_cast<scalar_t>(20.0f);
    if (x >  threshold) return x;            // log(1+e^x) ≈ x (large  x)
    if (x < -threshold) return expf(x);      // log(1+e^x) ≈ e^x (small x)
    return log1pf(expf(x));                  // stable path
}

// -----------------------------------------------------------------------------
// Mish activation: x * tanh(softplus(x))
// -----------------------------------------------------------------------------
template <typename scalar_t>
__device__ __forceinline__ scalar_t mish(const scalar_t x)
{
    return x * tanhf(softplus<scalar_t>(x));
}

// -----------------------------------------------------------------------------
// Fused kernel: y = mish(mish(x))
// -----------------------------------------------------------------------------
template <typename scalar_t>
__global__ void fused_double_mish_kernel(const scalar_t* __restrict__ inp,
                                         scalar_t*       __restrict__ out,
                                         size_t N)
{
    const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N)
    {
        scalar_t v = inp[idx];
        v = mish<scalar_t>(v);   // first Mish
        v = mish<scalar_t>(v);   // second Mish
        out[idx] = v;
    }
}

// -----------------------------------------------------------------------------
// Host launcher
// -----------------------------------------------------------------------------
torch::Tensor fused_double_mish(torch::Tensor x)
{
    TORCH_CHECK(x.is_cuda(),      "Input must be a CUDA tensor");
    TORCH_CHECK(x.scalar_type()==at::kFloat, "Only float32 supported");

    const size_t N       = static_cast<size_t>(x.numel());
    const int    threads = 256;
    const int    blocks  = (N + threads - 1) / threads;

    auto y = torch::empty_like(x);

    fused_double_mish_kernel<float><<<blocks, threads>>>(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        N);

    return y;
}
"""

cpp_src = "torch::Tensor fused_double_mish(torch::Tensor x);"

fused_ext = load_inline(
    name       = "fused_double_mish_ext",
    cpp_sources= cpp_src,
    cuda_sources = source,
    functions  = ["fused_double_mish"],
    verbose    = False,
)

class ModelNew(nn.Module):
    """
    Conv2d ➔ fused mish(mish(·))
    Spatial size shrinks by (kernel_size - 1) because padding = 0.
    """
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=0)
        self._mish = fused_ext

    def forward(self, x):
        x = self.conv(x)
        x = self._mish.fused_double_mish(x)
        return x