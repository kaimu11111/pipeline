```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ------------------------------------------------------------------
# Inline CUDA kernel: bias add + clamp + scale + clamp + divide
# ------------------------------------------------------------------
cuda_source = r"""
#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>

template <typename scalar_t>
__global__ void fused_bias_clamp_scale_kernel(const scalar_t* __restrict__ x,
                                              const scalar_t* __restrict__ bias,
                                              scalar_t* __restrict__ out,
                                              int64_t elements,
                                              int64_t C, int64_t HW,
                                              float scaling) {
    int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= elements) return;

    int64_t c = (idx / HW) % C;          // channel index for bias broadcast
    scalar_t val = x[idx] + bias[c];

    val = val < 0.f ? 0.f : (val > 1.f ? 1.f : val); // clamp to [0,1]
    val *= scaling;
    val = val < 0.f ? 0.f : (val > 1.f ? 1.f : val); // clamp again to [0,1]
    val /= scaling;

    out[idx] = val;
}

torch::Tensor fused_bias_clamp_scale(torch::Tensor x,
                                     torch::Tensor bias,
                                     double scaling) {
    TORCH_CHECK(x.is_cuda(), "Input tensor must be CUDA");
    TORCH_CHECK(bias.is_cuda(), "Bias tensor must be CUDA");
    TORCH_CHECK(x.scalar_type() == torch::kFloat32, "Only float32 tensors are supported");
    TORCH_CHECK(bias.scalar_type() == torch::kFloat32, "Only float32 tensors are supported");

    auto x_contig = x.contiguous();
    auto bias_contig = bias.contiguous();
    auto out = torch::empty_like(x_contig);

    int64_t N  = x_contig.size(0);
    int64_t C  = x_contig.size(1);
    int64_t H  = x_contig.size(2);
    int64_t W  = x_contig.size(3);
    int64_t HW = H * W;
    int64_t elements = x_contig.numel();

    const int threads = 256;
    const int blocks = (elements + threads - 1) / threads;

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    fused_bias_clamp_scale_kernel<float><<<blocks, threads, 0, stream>>>(
        x_contig.data_ptr<float>(),
        bias_contig.data_ptr<float>(),
        out.data_ptr<float>(),
        elements, C, HW,
        static_cast<float>(scaling)
    );

    return out;
}
"""

cpp_declarations = """
torch::Tensor fused_bias_clamp_scale(torch::Tensor x,
                                     torch::Tensor bias,
                                     double scaling);
"""

fused_ops = load_inline(
    name="fused_bias_clamp_scale_mod",
    cpp_sources=cpp_declarations,
    cuda_sources=cuda_source,
    functions=["fused_bias_clamp_scale"],
    verbose=False,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"]
)

# ------------------------------------------------------------------
# Optimized model using the fused CUDA kernel
# ------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimized model that fuses bias add, clamp, scale, clamp, and divide
    into a single custom CUDA kernel after the ConvTranspose2d layer.
    """
    def __init__(self, in_channels, out_channels, kernel_size,
                 stride, padding, output_padding, bias_shape, scaling_factor):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, output_padding=output_padding
        )
        self.bias = nn.Parameter(torch.randn(bias_shape, device="cuda", dtype=torch.float32))
        self.scaling_factor = float(scaling_factor)  # ensure Python float

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_ops.fused_bias_clamp_scale(x, self.bias, self.scaling_factor)
        return x
```