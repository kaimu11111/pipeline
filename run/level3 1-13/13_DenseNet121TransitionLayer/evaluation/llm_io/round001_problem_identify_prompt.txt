You are given:

ERROR_LOG:
/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Error building extension 'fused_pointwise_pool': [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=fused_pointwise_pool -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_pointwise_pool/main.cpp -o main.o 
[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=fused_pointwise_pool -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_pointwise_pool/cuda.cu -o cuda.cuda.o 
[3/3] c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_pointwise_pool.so
FAILED: fused_pointwise_pool.so 
c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_pointwise_pool.so
/usr/bin/ld: cuda.cuda.o: in function `PyInit_fused_pointwise_pool':
tmpxft_000162f8_00000000-6_cuda.cudafe1.cpp:(.text+0x1431): multiple definition of `PyInit_fused_pointwise_pool'; main.o:main.cpp:(.text+0x257): first defined here
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.

PyTorch reference (ground truth):

import torch
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self, num_input_features: int, num_output_features: int):
        """
        :param num_input_features: The number of input feature maps
        :param num_output_features: The number of output feature maps
        """
        super(Model, self).__init__()
        self.transition = nn.Sequential(
            nn.BatchNorm2d(num_input_features),
            nn.ReLU(inplace=True),
            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, bias=False),
            nn.AvgPool2d(kernel_size=2, stride=2)
        )

    def forward(self, x):
        """
        :param x: Input tensor of shape (batch_size, num_input_features, height, width)
        :return: Downsampled tensor with reduced number of feature maps
        """
        return self.transition(x)

batch_size = 32
num_input_features = 16
num_output_features = 32
height, width = 128, 128

def get_inputs():
    return [torch.rand(batch_size, num_input_features, height, width)]

def get_init_inputs():
    return [num_input_features, num_output_features]

CUDA candidate (to audit):

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------------
# Hand-written CUDA kernels
# ---------------------------------------------------------------------------
cuda_sources = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// ------------------------------------------------------------
// 1×1 Convolution + ReLU (point-wise) kernel
// ------------------------------------------------------------
template <typename scalar_t>
__global__ void conv1x1_relu_kernel(const scalar_t* __restrict__ input,
                                    const scalar_t* __restrict__ weight,
                                    scalar_t* __restrict__ output,
                                    int N, int C_in, int H, int W, int C_out) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int total = N * C_out * H * W;
    if (idx >= total) return;

    // Un-flatten the linear index
    int w =  idx % W;
    int h = (idx / W) % H;
    int co = (idx / (W * H)) % C_out;
    int n  =  idx / (W * H * C_out);

    // Pointer offsets
    const scalar_t* weight_ptr = weight + co * C_in;          // (C_out, C_in)
    const scalar_t* in_ptr     = input  + ((n * C_in * H) + h) * W + w;

    scalar_t acc = 0;
    #pragma unroll
    for (int ci = 0; ci < C_in; ++ci) {
        acc += in_ptr[ci * H * W] * weight_ptr[ci];
    }
    // Fused ReLU
    acc = acc > 0 ? acc : 0;
    output[idx] = acc;
}

// Host wrapper
torch::Tensor conv1x1_relu_cuda(torch::Tensor input, torch::Tensor weight) {
    TORCH_CHECK(input.is_cuda(),  "input must be a CUDA tensor");
    TORCH_CHECK(weight.is_cuda(), "weight must be a CUDA tensor");
    TORCH_CHECK(input.dtype() == weight.dtype(), "input/weight dtype mismatch");
    TORCH_CHECK(input.dim()  == 4, "input must be NCHW");
    TORCH_CHECK(weight.dim() == 2, "weight must be Cout x Cin");
    TORCH_CHECK(input.size(1) == weight.size(1), "Cin mismatch");
    const int N      = input.size(0);
    const int C_in   = input.size(1);
    const int H      = input.size(2);
    const int W      = input.size(3);
    const int C_out  = weight.size(0);

    auto output = torch::empty({N, C_out, H, W}, input.options());

    const int threads = 256;
    const int total   = N * C_out * H * W;
    const int blocks  = (total + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "conv1x1_relu_cuda", ([&] {
        conv1x1_relu_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            weight.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            N, C_in, H, W, C_out);
    }));
    return output;
}

// ------------------------------------------------------------
// 2×2 Average-Pooling (stride 2) kernel
// ------------------------------------------------------------
template <typename scalar_t>
__global__ void avgpool2x2_kernel(const scalar_t* __restrict__ input,
                                  scalar_t* __restrict__ output,
                                  int N, int C, int H, int W) {
    const int H_out = H >> 1;  // divide by 2
    const int W_out = W >> 1;
    const int idx   = blockIdx.x * blockDim.x + threadIdx.x;
    const int total = N * C * H_out * W_out;
    if (idx >= total) return;

    int w_out =  idx % W_out;
    int h_out = (idx / W_out) % H_out;
    int c     = (idx / (W_out * H_out)) % C;
    int n     =  idx / (W_out * H_out * C);

    // Map to top-left corner of the 2×2 window in the input
    const int h_in = h_out << 1;
    const int w_in = w_out << 1;

    const scalar_t* base = input + ((n * C + c) * H + h_in) * W + w_in;

    scalar_t v00 = base[0];
    scalar_t v01 = base[1];
    scalar_t v10 = base[W];
    scalar_t v11 = base[W + 1];

    output[idx] = static_cast<scalar_t>(0.25) * (v00 + v01 + v10 + v11);
}

// Host wrapper
torch::Tensor avgpool2x2_cuda(torch::Tensor input) {
    TORCH_CHECK(input.is_cuda(),  "input must be a CUDA tensor");
    TORCH_CHECK(input.dim() == 4, "input must be NCHW");
    TORCH_CHECK((input.size(2) & 1) == 0 && (input.size(3) & 1) == 0,
                "H and W must be even for 2x2 avg-pool");

    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    const int H_out = H >> 1;
    const int W_out = W >> 1;
    auto output = torch::empty({N, C, H_out, W_out}, input.options());

    const int threads = 256;
    const int total   = N * C * H_out * W_out;
    const int blocks  = (total + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "avgpool2x2_cuda", ([&] {
        avgpool2x2_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            N, C, H, W);
    }));
    return output;
}

// Declarations that PyTorch needs
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("conv1x1_relu_cuda", &conv1x1_relu_cuda, "1x1 Convolution + ReLU (CUDA)");
    m.def("avgpool2x2_cuda",   &avgpool2x2_cuda,   "2x2 AvgPool stride-2 (CUDA)");
}
"""

cpp_declarations = """
torch::Tensor conv1x1_relu_cuda(torch::Tensor input, torch::Tensor weight);
torch::Tensor avgpool2x2_cuda(torch::Tensor input);
"""

# Compile/Load the kernels
custom_ops = load_inline(
    name         = "fused_pointwise_pool",
    cpp_sources  = cpp_declarations,
    cuda_sources = cuda_sources,
    functions    = ["conv1x1_relu_cuda", "avgpool2x2_cuda"],
    verbose      = False
)

# ---------------------------------------------------------------------------
# Optimised model
# ---------------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimised version of the original model using custom CUDA kernels:
        BN -> (1×1 Conv + ReLU) -> 2×2 AvgPool (stride 2)
    The convolution and activation are fused, and pooling is custom.
    """
    def __init__(self, num_input_features: int, num_output_features: int):
        super().__init__()
        self.bn = nn.BatchNorm2d(num_input_features, affine=True, eps=1e-5)
        # 1×1 convolution weights (no bias, as in the original)
        self.weight = nn.Parameter(
            torch.empty(num_output_features, num_input_features)
        )
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # BatchNorm (uses cuDNN / native kernel)
        x = self.bn(x)
        # Fused 1×1 convolution + ReLU
        x = custom_ops.conv1x1_relu_cuda(x, self.weight)
        # 2×2 average pooling (stride 2)
        x = custom_ops.avgpool2x2_cuda(x)
        return x


# ---------------------------------------------------------------------------
# Convenience factory to match original API
# ---------------------------------------------------------------------------
import math
def get_inputs():
    # These dimensions should match the original sample sizes
    batch_size = 32
    num_input_features = 16
    height, width = 128, 128
    return [torch.rand(batch_size, num_input_features, height, width, device="cuda")]

def get_init_inputs():
    # Align with the original signature
    return [16, 32]


Follow the Rules and produce the JSON exactly in the specified format.