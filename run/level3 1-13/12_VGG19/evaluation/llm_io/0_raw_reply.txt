```python
import math
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ------------------------------------------------------------------
#            CUDA kernels: Linear      &     Linear + ReLU
# ------------------------------------------------------------------
cuda_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

////////////////////////////////////////////////////////////////////////////////
// Generic helpers
////////////////////////////////////////////////////////////////////////////////
template<bool FuseReLU>
__global__ void linear_kernel(const float* __restrict__ input,
                              const float* __restrict__ weight,
                              const float* __restrict__ bias,
                              float* __restrict__ output,
                              int N, int in_features, int out_features) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * out_features;
    if (idx >= total) return;

    int n       = idx / out_features;   // row  index in batch  (0 .. N-1)
    int out_f   = idx % out_features;   // col  index in output (0 .. out_features-1)

    const float* in_row     = input  + n      * in_features;
    const float* weight_row = weight + out_f * in_features;

    float val = bias[out_f];
    #pragma unroll 4
    for (int k = 0; k < in_features; ++k) {
        val += in_row[k] * weight_row[k];
    }
    if constexpr (FuseReLU) {
        val = val > 0.0f ? val : 0.0f;
    }
    output[idx] = val;
}

////////////////////////////////////////////////////////////////////////////////
// Host launcher helpers
////////////////////////////////////////////////////////////////////////////////
torch::Tensor launch_linear(torch::Tensor input,
                            torch::Tensor weight,
                            torch::Tensor bias,
                            bool fuse_relu) {

    TORCH_CHECK(input.is_cuda(),  "input  must be CUDA");
    TORCH_CHECK(weight.is_cuda(), "weight must be CUDA");
    TORCH_CHECK(bias.is_cuda(),   "bias   must be CUDA");
    TORCH_CHECK(input.dim()  == 2, "input   must be 2-D (N, in_features)");
    TORCH_CHECK(weight.dim() == 2, "weight  must be 2-D (out_features, in_features)");
    TORCH_CHECK(bias.dim()   == 1, "bias    must be 1-D (out_features)");
    TORCH_CHECK(weight.size(1) == input.size(1), "in_features mismatch");
    TORCH_CHECK(weight.size(0) == bias.size(0),  "out_features mismatch");

    const int  N            = input.size(0);
    const int  in_features  = input.size(1);
    const int  out_features = weight.size(0);

    auto output = torch::empty({N, out_features}, input.options());

    const int threads = 256;
    const int total   = N * out_features;
    const int blocks  = (total + threads - 1) / threads;

    if (fuse_relu) {
        linear_kernel<true><<<blocks, threads>>>(
            input.data_ptr<float>(),
            weight.data_ptr<float>(),
            bias.data_ptr<float>(),
            output.data_ptr<float>(),
            N, in_features, out_features);
    } else {
        linear_kernel<false><<<blocks, threads>>>(
            input.data_ptr<float>(),
            weight.data_ptr<float>(),
            bias.data_ptr<float>(),
            output.data_ptr<float>(),
            N, in_features, out_features);
    }
    return output;
}

////////////////////////////////////////////////////////////////////////////////
// Pythonâ€“exposed wrappers
////////////////////////////////////////////////////////////////////////////////
torch::Tensor linear_forward_cuda(torch::Tensor input,
                                  torch::Tensor weight,
                                  torch::Tensor bias) {
    return launch_linear(input, weight, bias, /*fuse_relu=*/false);
}

torch::Tensor linear_relu_forward_cuda(torch::Tensor input,
                                       torch::Tensor weight,
                                       torch::Tensor bias) {
    return launch_linear(input, weight, bias, /*fuse_relu=*/true);
}
"""

cpp_decls = """
torch::Tensor linear_forward_cuda(torch::Tensor input,
                                  torch::Tensor weight,
                                  torch::Tensor bias);
torch::Tensor linear_relu_forward_cuda(torch::Tensor input,
                                       torch::Tensor weight,
                                       torch::Tensor bias);
"""

# Compile & load the extension
_linear_ops = load_inline(
    name="linear_ops",
    cpp_sources=cpp_decls,
    cuda_sources=cuda_source,
    functions=[
        "linear_forward_cuda",
        "linear_relu_forward_cuda",
    ],
    verbose=False,
)

# ------------------------------------------------------------------
#           Convenience PyTorch modules wrapping the kernels
# ------------------------------------------------------------------
class _BaseLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.bias   = nn.Parameter(torch.empty(out_features))
        self.reset_parameters()

    def reset_parameters(self):
        # Same init scheme as nn.Linear
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in)
        nn.init.uniform_(self.bias, -bound, bound)


class Linear(_BaseLinear):
    def forward(self, x):
        # Ensure contiguous before dispatching to CUDA kernel
        return _linear_ops.linear_forward_cuda(x.contiguous(), self.weight, self.bias)


class LinearReLU(_BaseLinear):
    def forward(self, x):
        return _linear_ops.linear_relu_forward_cuda(x.contiguous(), self.weight, self.bias)

# ------------------------------------------------------------------
#                   Optimised VGG-19  (ModelNew)
# ------------------------------------------------------------------
class ModelNew(nn.Module):
    def __init__(self, num_classes=1000):
        super().__init__()
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64,  kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 2
            nn.Conv2d(64, 128,  kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 3
            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 4
            nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # Block 5
            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.classifier = nn.Sequential(
            LinearReLU(512 * 7 * 7, 4096),
            LinearReLU(4096, 4096),
            Linear(4096, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x
```