You are a senior CUDA-extension developer.
Your job is to **FIX** the compilation or runtime errors in the Python script
shown below.

OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
2. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

────────────────────────────────────────
ERROR LOG
────────────────────────────────────────
Traceback (most recent call last):
  File "/home/wan00559/pipleline/utils/compile_and_run.py", line 524, in compare_and_bench
    ref_out,  _ = _run_once(ref_model,  inp, dev)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wan00559/pipleline/utils/compile_and_run.py", line 127, in _run_once
    out = model(*inp)
          ^^^^^^^^^^^
  File "/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wan00559/pipleline/KernelBench/level3test/5_AlexNet.py", line 71, in forward
    x = self.fc1(x)
        ^^^^^^^^^^^
  File "/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 shapes cannot be multiplied (512x1024 and 9216x4096)

────────────────────────────────────────
OLD CODE (read-only)
────────────────────────────────────────
# 1. Imports ──────────────────────────────────────────────────────────
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# 2. source ───────────────────────────────────────────────────────────
source = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

////////////////////////////////////////////////////////////////
// kernel
////////////////////////////////////////////////////////////////
__global__ void linear_relu_forward_kernel(
        const float* __restrict__ A,      // (batch, in_features)
        const float* __restrict__ W,      // (out_features, in_features)
        const float* __restrict__ B,      // (out_features)
        float* __restrict__ Y,            // (batch, out_features)
        int batch,
        int in_features,
        int out_features)
{
    const int row = blockIdx.y * blockDim.y + threadIdx.y;   // batch index
    const int col = blockIdx.x * blockDim.x + threadIdx.x;   // output index

    if (row < batch && col < out_features)
    {
        float acc = 0.f;
        const float* a_ptr = A + row * in_features;          // ptr to row in A
        const float* w_ptr = W + col * in_features;          // ptr to row in W
        for (int k = 0; k < in_features; ++k)
            acc += a_ptr[k] * w_ptr[k];

        acc += B[col];
        Y[row * out_features + col] = fmaxf(acc, 0.f);       // ReLU
    }
}

////////////////////////////////////////////////////////////////
// C++/CUDA launcher
////////////////////////////////////////////////////////////////
torch::Tensor linear_relu_forward_cuda(torch::Tensor A,
                                       torch::Tensor W,
                                       torch::Tensor B)
{
    TORCH_CHECK(A.is_cuda() && W.is_cuda() && B.is_cuda(),
                "All tensors must be CUDA");
    TORCH_CHECK(A.dtype() == torch::kFloat32 &&
                W.dtype() == torch::kFloat32 &&
                B.dtype() == torch::kFloat32,
                "Only float32 supported");

    const int batch        = A.size(0);
    const int in_features  = A.size(1);
    const int out_features = W.size(0);

    auto Y = torch::empty({batch, out_features}, A.options());

    dim3 block(16, 16);
    dim3 grid((out_features + block.x - 1) / block.x,
              (batch        + block.y - 1) / block.y);

    linear_relu_forward_kernel<<<grid, block>>>(
        A.data_ptr<float>(),
        W.data_ptr<float>(),
        B.data_ptr<float>(),
        Y.data_ptr<float>(),
        batch,
        in_features,
        out_features);

    return Y;
}
"""

# 3. cpp_src ──────────────────────────────────────────────────────────
cpp_src = """
torch::Tensor linear_relu_forward_cuda(torch::Tensor A,
                                       torch::Tensor W,
                                       torch::Tensor B);
"""

# 4. load_inline ──────────────────────────────────────────────────────
linear_relu_ext = load_inline(
    name="linear_relu_ext",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["linear_relu_forward_cuda"],
    verbose=False
)

# 5. class ModelNew ───────────────────────────────────────────────────
class ModelNew(nn.Module):
    """
    AlexNet-like network whose first two fully-connected layers are served by
    a custom fused Linear+ReLU CUDA kernel.
    """

    # ---------------- Nested helpers ---------------------------------
    class _LinearReLUFunction(torch.autograd.Function):
        @staticmethod
        def forward(ctx, inp, weight, bias):
            inp_c    = inp.contiguous()
            weight_c = weight.contiguous()
            bias_c   = bias.contiguous()
            out = linear_relu_ext.linear_relu_forward_cuda(inp_c, weight_c, bias_c)
            ctx.save_for_backward(inp_c, weight_c, bias_c, out)
            return out

        @staticmethod
        def backward(ctx, grad_out):
            inp, weight, bias, out = ctx.saved_tensors
            grad = grad_out.clone()
            grad[out == 0] = 0                                           # ReLU mask
            grad_inp    = grad.matmul(weight)                            # (B, in_feat)
            grad_weight = grad.t().matmul(inp)                           # (out_feat, in_feat)
            grad_bias   = grad.sum(0)                                    # (out_feat)
            return grad_inp, grad_weight, grad_bias

    class _FusedLinearReLU(nn.Module):
        def __init__(self, in_features, out_features):
            super().__init__()
            self.weight = nn.Parameter(torch.empty(out_features, in_features))
            self.bias   = nn.Parameter(torch.empty(out_features))
            self.reset_parameters()

        def reset_parameters(self):
            nn.init.kaiming_uniform_(self.weight, a=5 ** 0.5)
            fan_in = self.weight.size(1)
            bound  = 1.0 / (fan_in ** 0.5)
            nn.init.uniform_(self.bias, -bound, bound)

        def forward(self, x):
            return ModelNew._LinearReLUFunction.apply(x, self.weight, self.bias)

    # ---------------- Model definition -------------------------------
    def __init__(self, num_classes: int = 1000):
        super().__init__()

        # Convolutional backbone
        self.conv1     = nn.Conv2d(3,   96, 11, stride=4, padding=2)
        self.relu1     = nn.ReLU(inplace=True)
        self.maxpool1  = nn.MaxPool2d(3, stride=2)

        self.conv2     = nn.Conv2d(96, 256, 5, padding=2)
        self.relu2     = nn.ReLU(inplace=True)
        self.maxpool2  = nn.MaxPool2d(3, stride=2)

        self.conv3     = nn.Conv2d(256, 384, 3, padding=1)
        self.relu3     = nn.ReLU(inplace=True)

        self.conv4     = nn.Conv2d(384, 384, 3, padding=1)
        self.relu4     = nn.ReLU(inplace=True)

        self.conv5     = nn.Conv2d(384, 256, 3, padding=1)
        self.relu5     = nn.ReLU(inplace=True)
        self.maxpool3  = nn.MaxPool2d(3, stride=2)

        # Fused fully-connected layers
        #   Flatten after convs gives 256×2×2 = 1024 features for the
        #   dataset size used by the benchmark harness.
        self.fc1       = ModelNew._FusedLinearReLU(256 * 2 * 2, 4096)
        self.dropout1  = nn.Dropout(p=0.0)

        self.fc2       = ModelNew._FusedLinearReLU(4096, 4096)
        self.dropout2  = nn.Dropout(p=0.0)

        # Final classifier
        self.fc3       = nn.Linear(4096, num_classes)

    # ---------------- Forward ----------------------------------------
    def forward(self, x):
        x = self.maxpool1(self.relu1(self.conv1(x)))
        x = self.maxpool2(self.relu2(self.conv2(x)))
        x = self.relu3(self.conv3(x))
        x = self.relu4(self.conv4(x))
        x = self.maxpool3(self.relu5(self.conv5(x)))
        x = torch.flatten(x, 1)
        x = self.dropout1(self.fc1(x))
        x = self.dropout2(self.fc2(x))
        x = self.fc3(x)
        return x

────────────────────────────────────────
Main Critical Problem
────────────────────────────────────────
critical_issue: fc1 built for 256*2*2=1024, ref needs 256*6*6=9216
why_it_matters: Dimension mismatch changes parameters and activations; outputs diverge and reference even crashes during matmul.
minimal_fix_hint: define fc1 with 9216 in_features

```python
# <your corrected code>
```
# ==========================================================
