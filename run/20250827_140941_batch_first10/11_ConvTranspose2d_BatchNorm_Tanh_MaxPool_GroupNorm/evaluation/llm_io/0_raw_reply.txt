```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

##############################################
# Fused tanh + 2×2 max-pool CUDA kernel
##############################################
cuda_src = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void tanh_maxpool2d_kernel(const float* __restrict__ in,
                                      float* __restrict__ out,
                                      const int N,
                                      const int C,
                                      const int H_in,
                                      const int W_in) {
    const int H_out = H_in >> 1;   // divide by 2
    const int W_out = W_in >> 1;
    const int total_elems = N * C * H_out * W_out;

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elems) return;

    const int w_out = idx % W_out;
    const int h_out = (idx / W_out) % H_out;
    const int c     = (idx / (W_out * H_out)) % C;
    const int n     =  idx / (W_out * H_out * C);

    const int h_in = h_out << 1;   // multiply by 2
    const int w_in = w_out << 1;

    const int in_offset  = (((n * C + c) * H_in + h_in) * W_in + w_in);

    // 2×2 window – explicit offsets to maximise throughput
    const float v0 = tanhf(in[in_offset]);
    const float v1 = tanhf(in[in_offset + 1]);
    const float v2 = tanhf(in[in_offset + W_in]);
    const float v3 = tanhf(in[in_offset + W_in + 1]);

    float m = fmaxf(fmaxf(v0, v1), fmaxf(v2, v3));
    out[idx] = m;
}

torch::Tensor tanh_maxpool2d_fusion(torch::Tensor x) {
    TORCH_CHECK(x.is_cuda(), "Input must reside on CUDA device");
    TORCH_CHECK(x.dim() == 4, "Expected 4-D tensor (N,C,H,W)");
    const int64_t N = x.size(0);
    const int64_t C = x.size(1);
    const int64_t H = x.size(2);
    const int64_t W = x.size(3);
    TORCH_CHECK(H % 2 == 0 && W % 2 == 0,
                "Input spatial dims must be even for 2×2 pool");

    auto y = torch::empty({N, C, H / 2, W / 2}, x.options());

    const int threads = 256;
    const int blocks  = (y.numel() + threads - 1) / threads;

    tanh_maxpool2d_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        (int)N, (int)C, (int)H, (int)W);

    cudaError_t err = cudaGetLastError();
    TORCH_CHECK(err == cudaSuccess, "CUDA kernel failed : ",
                cudaGetErrorString(err));

    return y;
}
"""

cpp_src = "torch::Tensor tanh_maxpool2d_fusion(torch::Tensor x);"

# Compile / load
tanh_maxpool_mod = load_inline(
    name="tanh_maxpool_fused",
    cpp_sources=cpp_src,
    cuda_sources=cuda_src,
    functions=["tanh_maxpool2d_fusion"],
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"],
    verbose=False,
)

##############################################
# Autograd-wrapper (forward only)
##############################################
class _FusedTanhMaxPool(torch.autograd.Function):
    @staticmethod
    def forward(ctx, inp):
        return tanh_maxpool_mod.tanh_maxpool2d_fusion(inp)

    @staticmethod
    def backward(ctx, grad_out):
        # Backward path not implemented (use .eval() / inference)
        raise RuntimeError("Backward not implemented for fused tanh-maxpool")

def fused_tanh_maxpool2d(x: torch.Tensor) -> torch.Tensor:
    return _FusedTanhMaxPool.apply(x)

##############################################
# Optimised model using the fused kernel
##############################################
class ModelNew(nn.Module):
    """
    Optimised model that keeps heavyweight layers in PyTorch but replaces
    the tanh activation + 2×2 max-pool sequence with a single custom
    CUDA kernel.
    """
    def __init__(self, in_channels, out_channels,
                 kernel_size, stride, padding,
                 groups, num_groups):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels,
            kernel_size, stride=stride,
            padding=padding)
        self.batch_norm = nn.BatchNorm2d(out_channels)
        self.group_norm = nn.GroupNorm(num_groups=num_groups,
                                       num_channels=out_channels)

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = fused_tanh_maxpool2d(x)
        x = self.group_norm(x)
        return x

##############################################
# Helpers (unchanged)
##############################################
batch_size   = 256
in_channels  = 32
out_channels = 64
height = width = 32
kernel_size  = 5
stride       = 1
padding      = 1
groups       = 8
num_groups   = 8

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width, device='cuda')]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size,
            stride, padding, groups, num_groups]
```