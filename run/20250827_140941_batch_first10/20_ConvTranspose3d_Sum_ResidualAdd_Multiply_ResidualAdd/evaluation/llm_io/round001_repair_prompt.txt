You are a senior CUDA-extension developer.
Your job is to **FIX** the compilation or runtime errors in the Python script
shown below.

OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
2. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

────────────────────────────────────────
ERROR LOG
────────────────────────────────────────
Traceback (most recent call last):
  File "/home/wan00559/pipleline/utils/compile_and_run.py", line 541, in compare_and_bench
    raise ValueError(
ValueError: Outputs are not close (atol=0.001, rtol=0.001). max_abs_err=nan, mean_abs_err=nan

────────────────────────────────────────
OLD CODE (read-only)
────────────────────────────────────────
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline


# ---------------------------------------------------------------------------
# CUDA implementation: fused bias-add + three residual ops (see description)
# Forward  : y = ((x + bias) + x.detach()) * x.detach() + x.detach()
#            == (2*x + bias) * x + x
# Backward : dy/dx    = x.detach()
#            dy/dbias = x.detach()
# ---------------------------------------------------------------------------

cuda_src = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void fused_forward_kernel(const scalar_t* __restrict__ x,
                                     const scalar_t* __restrict__ bias,
                                     scalar_t* __restrict__ y,
                                     const int C,
                                     const int inner) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int total = C * inner;
    if (idx >= total) return;

    const int c = (idx / inner) % C;          // channel index
    const scalar_t b = bias[c];
    const scalar_t val = x[idx];

    // y = (2*val + b) * val + val
    y[idx] = (static_cast<scalar_t>(2) * val + b) * val + val;
}

torch::Tensor fused_forward(torch::Tensor x, torch::Tensor bias) {
    TORCH_CHECK(x.is_cuda(), "input must reside on CUDA");
    TORCH_CHECK(bias.is_cuda(), "bias must reside on CUDA");
    TORCH_CHECK(x.is_contiguous(), "input must be contiguous");
    TORCH_CHECK(bias.is_contiguous(), "bias must be contiguous");

    const int64_t N = x.size(0);
    const int64_t C = x.size(1);
    const int64_t inner = x.size(2) * x.size(3) * x.size(4);   // D*H*W
    const int64_t total = N * C * inner;

    auto y = torch::empty_like(x);

    const int threads = 256;
    const int blocks  = (total + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(
        x.scalar_type(), "fused_forward_kernel", ([&] {
            fused_forward_kernel<scalar_t><<<blocks, threads>>>(
                x.data_ptr<scalar_t>(),
                bias.data_ptr<scalar_t>(),
                y.data_ptr<scalar_t>(),
                static_cast<int>(C),
                static_cast<int>(inner));
        }));

    return y;
}
"""

cpp_src = "torch::Tensor fused_forward(torch::Tensor x, torch::Tensor bias);"

# compile the CUDA extension at import time
fused_ops = load_inline(
    name="fused_bias_residual",
    cpp_sources=cpp_src,
    cuda_sources=cuda_src,
    functions=["fused_forward"],
    verbose=False,
)

# ---------------------------------------------------------------------------
# Autograd wrapper for fused kernel
# ---------------------------------------------------------------------------
class _FusedBiasResidualFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x: torch.Tensor, bias: torch.Tensor):
        # save detached input for backward
        ctx.save_for_backward(x.detach())
        ctx.in_channels = x.size(1)
        y = fused_ops.fused_forward(x.contiguous(), bias.contiguous())
        return y

    @staticmethod
    def backward(ctx, grad_output):
        (x_detached,) = ctx.saved_tensors

        # grad w.r.t. input
        grad_input = grad_output * x_detached

        # grad w.r.t. bias (broadcasted across spatial dims)
        axes = (0, 2, 3, 4)  # sum across N, D, H, W
        grad_bias = (grad_output * x_detached).sum(dim=axes)

        # reshape grad_bias to match the Parameter's original shape (C,1,1,1)
        grad_bias = grad_bias.view(ctx.in_channels, 1, 1, 1)

        return grad_input, grad_bias


# handy alias
fused_bias_residual = _FusedBiasResidualFunction.apply


# ---------------------------------------------------------------------------
# Optimised model
# ---------------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimised version of the original model using a fused custom CUDA kernel
    to perform:
      1) bias add
      2) residual add
      3) element-wise multiplication
      4) final residual add
    in a single pass.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride,
                 padding, output_padding, bias_shape):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size, stride=stride,
            padding=padding, output_padding=output_padding
        )
        # keep original bias shape (C,1,1,1) for broadcasting convenience
        self.bias = nn.Parameter(torch.randn(*bias_shape))

    def forward(self, x):
        x = self.conv_transpose(x)
        # fused post-processing
        x = fused_bias_residual(x, self.bias)
        return x


# ---------------------------------------------------------------------------
# Helpers (kept identical to original spec)
# ---------------------------------------------------------------------------
batch_size = 8
in_channels = 16
out_channels = 32
depth, height, width = 8, 16, 16
kernel_size = 3
stride = 2
padding = 1
output_padding = 1
bias_shape = (out_channels, 1, 1, 1)


def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width, device="cuda")]


def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding,
            output_padding, bias_shape]

────────────────────────────────────────
Main Critical Problem
────────────────────────────────────────
critical_issue: Kernel processes only C*inner, ignores batch N
why_it_matters: Threads with idx >= C*inner (all later batches) exit early, leaving y uninitialized → garbage/NaNs and wrong outputs.
minimal_fix_hint: Pass N and use total = N*C*inner in kernel

```python
# <your corrected code>
```
# ==========================================================
