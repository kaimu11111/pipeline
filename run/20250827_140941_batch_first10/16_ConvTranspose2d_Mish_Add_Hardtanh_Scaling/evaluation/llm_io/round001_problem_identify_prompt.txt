You are given:

ERROR_LOG:
/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Error building extension 'fused_ops': [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=fused_ops -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_ops/main.cpp -o main.o 
[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=fused_ops -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_ops/cuda.cu -o cuda.cuda.o 
[3/3] c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_ops.so
FAILED: fused_ops.so 
c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_ops.so
/usr/bin/ld: cuda.cuda.o: in function `PyInit_fused_ops':
tmpxft_003cc06c_00000000-6_cuda.cudafe1.cpp:(.text+0x3ff): multiple definition of `PyInit_fused_ops'; main.o:main.cpp:(.text+0x257): first defined here
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.

PyTorch reference (ground truth):

import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a transposed convolution, applies Mish activation, adds a value, 
    applies Hardtanh activation, and scales the output.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale):
        super(Model, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)
        self.add_value = add_value
        self.scale = scale

    def forward(self, x):
        x = self.conv_transpose(x)
        x = torch.nn.functional.mish(x) # Mish activation
        x = x + self.add_value
        x = torch.nn.functional.hardtanh(x, min_val=-1, max_val=1) # Hardtanh activation
        x = x * self.scale # Scaling
        return x

batch_size = 64
in_channels  = 32  
out_channels = 32  
height = width = 64  
kernel_size  = 3
stride       = 2  
padding      = 1
output_padding = 1
add_value = 0.5
scale = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale]

CUDA candidate (to audit):

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ------------------------------------------------------------------
# Build the fused CUDA kernel (Mish → add → Hardtanh → scale)
# ------------------------------------------------------------------
cpp_src = r"""
#include <torch/extension.h>
torch::Tensor fused_forward_cuda(torch::Tensor input, double add_val, double scale_val);
"""

cuda_src = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__device__ __forceinline__ float mish(float x) {
    // Numerically-stable softplus: log(1+e^x)
    float sp = logf(1.0f + expf(-fabsf(x))) + fmaxf(x, 0.0f);
    return x * tanhf(sp);
}

__global__ void fused_kernel(const float* __restrict__ in,
                             float* __restrict__ out,
                             int64_t size,
                             float add_val,
                             float scale_val) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= size) return;

    float y = mish(in[idx]);             // Mish
    y += add_val;                        // + constant
    y = fminf(fmaxf(y, -1.0f), 1.0f);    // Hardtanh clamp
    y *= scale_val;                      // scale
    out[idx] = y;
}

torch::Tensor fused_forward_cuda(torch::Tensor input,
                                 double add_val_d,
                                 double scale_val_d) {
    float add_val   = static_cast<float>(add_val_d);
    float scale_val = static_cast<float>(scale_val_d);

    auto output = torch::empty_like(input);
    const int64_t size = input.numel();

    const int threads = 256;
    const int blocks  = (size + threads - 1) / threads;

    fused_kernel<<<blocks, threads>>>(input.data_ptr<float>(),
                                      output.data_ptr<float>(),
                                      size,
                                      add_val,
                                      scale_val);
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("fused_forward_cuda", &fused_forward_cuda,
          "Fused Mish + Add + Hardtanh + Scale (CUDA)");
}
"""

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=cpp_src,
    cuda_sources=cuda_src,
    functions=["fused_forward_cuda"],
    verbose=False,
)

# ------------------------------------------------------------------
# Optimized model
# ------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Model that performs a transposed convolution and a fused CUDA kernel
    implementing: Mish → add → Hardtanh → scale
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride,
                 padding, output_padding, add_value, scale):
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size,
            stride, padding, output_padding
        )
        # Store constants as buffers so they move with .cuda(), .to(device), etc.
        self.register_buffer("add_value_tensor",
                             torch.tensor(float(add_value), dtype=torch.float32))
        self.register_buffer("scale_tensor",
                             torch.tensor(float(scale), dtype=torch.float32))
        self._fused = fused_ops

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x.contiguous()  # guarantee contiguous memory layout
        x = self._fused.fused_forward_cuda(
            x,
            float(self.add_value_tensor.item()),
            float(self.scale_tensor.item()),
        )
        return x

# ------------------------------------------------------------------
# Helpers matching the original API
# ------------------------------------------------------------------
batch_size = 64
in_channels  = 32
out_channels = 32
height = width = 64
kernel_size  = 3
stride       = 2
padding      = 1
output_padding = 1
add_value = 0.5
scale = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width, device='cuda')]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride,
            padding, output_padding, add_value, scale]


Follow the Rules and produce the JSON exactly in the specified format.