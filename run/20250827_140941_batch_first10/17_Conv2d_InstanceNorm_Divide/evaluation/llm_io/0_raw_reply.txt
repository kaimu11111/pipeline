```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ------------------------------------------------------------------
# CUDA / C++: fused InstanceNorm2d + divide-by-constant
# ------------------------------------------------------------------
cuda_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <vector>

// ------------------------------------------------------------------
// CUDA kernel
// ------------------------------------------------------------------
__global__ void instancenorm_div_kernel(const float* __restrict__ input,
                                        float* __restrict__ output,
                                        const float* __restrict__ weight,
                                        const float* __restrict__ bias,
                                        const int C,
                                        const int H,
                                        const int W,
                                        const float eps,
                                        const float inv_div_by)
{
    const int HW = H * W;
    const int nc  = blockIdx.x;     // one block per (N,C) tuple
    const int c   = nc % C;
    const int n   = nc / C;

    const float* x_ptr = input  + nc * HW;
    float*       y_ptr = output + nc * HW;

    // ------------------------------------------------------------------
    // 1) compute mean & variance across (H*W) for this (n,c)
    // ------------------------------------------------------------------
    extern __shared__ float shm[];        // size = 2 * blockDim.x
    float* shm_sum   = shm;               // first  blockDim.x floats
    float* shm_sqsum = shm + blockDim.x;  // second blockDim.x floats

    float sum   = 0.0f;
    float sqsum = 0.0f;

    for (int idx = threadIdx.x; idx < HW; idx += blockDim.x) {
        float val = x_ptr[idx];
        sum   += val;
        sqsum += val * val;
    }

    shm_sum[threadIdx.x]   = sum;
    shm_sqsum[threadIdx.x] = sqsum;
    __syncthreads();

    // parallel reduction
    for (int stride = blockDim.x >> 1; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shm_sum[threadIdx.x]   += shm_sum[threadIdx.x + stride];
            shm_sqsum[threadIdx.x] += shm_sqsum[threadIdx.x + stride];
        }
        __syncthreads();
    }

    const float mean = shm_sum[0] / HW;
    const float var  = shm_sqsum[0] / HW - mean * mean;
    const float inv_std = rsqrtf(var + eps);

    // scale/shift (weight,bias) + divide-by
    const float gamma = (weight != nullptr ? weight[c] : 1.0f) * inv_div_by;
    const float beta  = (bias  != nullptr ? bias[c]  : 0.0f) * inv_div_by;

    // ------------------------------------------------------------------
    // 2) normalization + affine + divide
    // ------------------------------------------------------------------
    for (int idx = threadIdx.x; idx < HW; idx += blockDim.x) {
        float norm = (x_ptr[idx] - mean) * inv_std;   // instance-norm
        norm = norm * gamma + beta;                   // affine & divide
        y_ptr[idx] = norm;
    }
}

// ------------------------------------------------------------------
// C++/CUDA launcher
// ------------------------------------------------------------------
torch::Tensor instancenorm_div_cuda(torch::Tensor input,
                                    c10::optional<torch::Tensor> weight_opt,
                                    c10::optional<torch::Tensor> bias_opt,
                                    const float  eps,
                                    const float  divide_by)
{
    TORCH_CHECK(input.is_cuda(), "input must be a CUDA tensor");
    TORCH_CHECK(input.dtype() == torch::kFloat,
                "only float32 tensors are supported");

    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto output = torch::empty_like(input);

    const int blocks  = N * C;     // one block per (n,c)
    const int threads = 256;
    const size_t shmem = threads * 2 * sizeof(float);

    const float inv_div_by = 1.0f / divide_by;

    const float* weight_ptr = nullptr;
    const float* bias_ptr   = nullptr;

    if (weight_opt.has_value())
        weight_ptr = weight_opt.value().data_ptr<float>();
    if (bias_opt.has_value())
        bias_ptr = bias_opt.value().data_ptr<float>();

    instancenorm_div_kernel<<<blocks, threads, shmem>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        weight_ptr,
        bias_ptr,
        C, H, W,
        eps,
        inv_div_by);

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
        TORCH_CHECK(false, "CUDA kernel failed : ",
                    cudaGetErrorString(err));

    return output;
}

// ------------------------------------------------------------------
// PyBind
// ------------------------------------------------------------------
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("instancenorm_div_cuda", &instancenorm_div_cuda,
          "Fused InstanceNorm2d + divide (CUDA)");
}
"""

# header with declaration for jit
cpp_decl = "torch::Tensor instancenorm_div_cuda(torch::Tensor input, c10::optional<torch::Tensor> weight, c10::optional<torch::Tensor> bias, float eps, float divide_by);"

fused_instancenorm_div = load_inline(
    name="fused_instancenorm_div",
    cpp_sources=cpp_decl,
    cuda_sources=cuda_source,
    functions=["instancenorm_div_cuda"],
    verbose=False,
)

# ------------------------------------------------------------------
# Python-side wrapper module
# ------------------------------------------------------------------
class FusedInstanceNormDiv(nn.Module):
    def __init__(self, num_features, eps=1e-5, divide_by=1.0):
        super().__init__()
        self.eps = eps
        self.divide_by = float(divide_by)

        # Learnable affine parameters, same as nn.InstanceNorm2d
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias   = nn.Parameter(torch.zeros(num_features))

    def forward(self, x):
        return fused_instancenorm_div.instancenorm_div_cuda(
            x,
            self.weight,
            self.bias,
            self.eps,
            self.divide_by
        )

# ------------------------------------------------------------------
# Optimised model
# ------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimised model with fused InstanceNorm2d + divide-by-constant
    custom CUDA kernel.
    """
    def __init__(self, in_channels, out_channels, kernel_size, divide_by):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.inst_div = FusedInstanceNormDiv(out_channels,
                                             eps=1e-5,
                                             divide_by=divide_by)

    def forward(self, x):
        x = self.conv(x)
        x = self.inst_div(x)
        return x
```