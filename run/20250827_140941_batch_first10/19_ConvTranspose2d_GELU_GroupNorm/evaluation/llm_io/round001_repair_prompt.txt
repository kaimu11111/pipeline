You are a senior CUDA-extension developer.
Your job is to **FIX** the compilation or runtime errors in the Python script
shown below.

OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
2. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

────────────────────────────────────────
ERROR LOG
────────────────────────────────────────
Traceback (most recent call last):
  File "/home/wan00559/pipleline/utils/compile_and_run.py", line 541, in compare_and_bench
    raise ValueError(
ValueError: Outputs are not close (atol=0.001, rtol=0.001). max_abs_err=3.090e-03, mean_abs_err=9.786e-05

────────────────────────────────────────
OLD CODE (read-only)
────────────────────────────────────────
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------
# Custom CUDA implementation of GELU (approximation) – element-wise
# ---------------------------------------------------------------------
cuda_src = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__device__ __forceinline__ scalar_t gelu_device(const scalar_t x) {
    const scalar_t kAlpha = 0.7978845608028654;          // sqrt(2/pi)
    return static_cast<scalar_t>(0.5) * x *
           (static_cast<scalar_t>(1.0) +
            tanh(kAlpha * (x + static_cast<scalar_t>(0.044715) * x * x * x)));
}

template <typename scalar_t>
__global__ void gelu_forward_kernel(const scalar_t* __restrict__ in,
                                    scalar_t* __restrict__ out,
                                    size_t numel) {
    const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < numel) {
        out[idx] = gelu_device(in[idx]);
    }
}

torch::Tensor gelu_forward_cuda(torch::Tensor input) {
    const size_t numel = input.numel();
    auto output = torch::empty_like(input);

    const int threads = 256;
    const int blocks  = (numel + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES(
        input.scalar_type(), "gelu_forward_cuda", ([&] {
            gelu_forward_kernel<scalar_t><<<blocks, threads>>>(
                input.data_ptr<scalar_t>(),
                output.data_ptr<scalar_t>(),
                numel);
        }));

    return output;
}
"""

cpp_src = "torch::Tensor gelu_forward_cuda(torch::Tensor input);"

gelu_mod = load_inline(
    name="custom_gelu_cuda",
    cpp_sources=cpp_src,
    cuda_sources=cuda_src,
    functions=["gelu_forward_cuda"],
    verbose=False,
)

# ---------------------------------------------------------------------
# Optimised model using the custom CUDA GELU
# ---------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Model that performs a transposed convolution, applies a custom-CUDA GELU,
    and normalizes with GroupNorm.
    """
    def __init__(self, in_channels, out_channels,
                 kernel_size, stride, groups, num_groups):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size, stride=stride
        )
        self.group_norm = nn.GroupNorm(num_groups=num_groups,
                                       num_channels=out_channels)
        self._gelu = gelu_mod.gelu_forward_cuda

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self._gelu(x)
        x = self.group_norm(x)
        return x

────────────────────────────────────────
Main Critical Problem
────────────────────────────────────────
critical_issue: Implements tanh-approx GELU, but reference uses exact GELU
why_it_matters: Approximate formula shifts outputs; observed max_abs_err 3e-3 exceeds 1e-3 tolerance, diverging model behaviour and gradients.
minimal_fix_hint: Switch to exact GELU formula or enable PyTorch approximate='tanh'.

```python
# <your corrected code>
```
# ==========================================================
