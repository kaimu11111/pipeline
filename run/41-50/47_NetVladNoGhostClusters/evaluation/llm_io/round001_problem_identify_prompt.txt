You are given:

ERROR_LOG:
/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Error building extension 'vlad_custom_ops': [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=vlad_custom_ops -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/vlad_custom_ops/main.cpp -o main.o 
[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=vlad_custom_ops -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/vlad_custom_ops/cuda.cu -o cuda.cuda.o 
[3/3] c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o vlad_custom_ops.so
FAILED: vlad_custom_ops.so 
c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o vlad_custom_ops.so
/usr/bin/ld: cuda.cuda.o: in function `PyInit_vlad_custom_ops':
tmpxft_0001949b_00000000-6_cuda.cudafe1.cpp:(.text+0x6e1): multiple definition of `PyInit_vlad_custom_ops'; main.o:main.cpp:(.text+0x257): first defined here
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.

PyTorch reference (ground truth):

# Copyright 2018 Antoine Miech All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS-IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Code modified from here
https://github.com/albanie/collaborative-experts/blob/master/model/net_vlad.py
"""


import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch as th


class Model(nn.Module):
    def __init__(self, cluster_size, feature_size, ghost_clusters):
        super(Model, self).__init__()

        self.feature_size = feature_size
        self.cluster_size = cluster_size
        self.ghost_clusters = ghost_clusters

        init_sc = (1 / math.sqrt(feature_size))
        clusters = cluster_size + ghost_clusters

        # The `clusters` weights are the `(w,b)` in the paper
        self.clusters = nn.Parameter(init_sc * th.randn(feature_size, clusters))
        self.batch_norm = nn.BatchNorm1d(clusters)
        # The `clusters2` weights are the visual words `c_k` in the paper
        self.clusters2 = nn.Parameter(init_sc * th.randn(1, feature_size, cluster_size))
        self.out_dim = self.cluster_size * feature_size

    def forward(self, x, mask=None):
        """Aggregates feature maps into a fixed size representation.  In the following
        notation, B = batch_size, N = num_features, K = num_clusters, D = feature_size.

        Args:
            x (th.Tensor): B x N x D

        Returns:
            (th.Tensor): B x DK
        """
        max_sample = x.size()[1]
        x = x.view(-1, self.feature_size)  # B x N x D -> BN x D

        if x.device != self.clusters.device:
            msg = f"x.device {x.device} != cluster.device {self.clusters.device}"
            raise ValueError(msg)

        assignment = th.matmul(x, self.clusters)  # (BN x D) x (D x (K+G)) -> BN x (K+G)
        assignment = self.batch_norm(assignment)

        assignment = F.softmax(assignment, dim=1)  # BN x (K+G) -> BN x (K+G)
        # remove ghost assigments
        assignment = assignment[:, :self.cluster_size]
        assignment = assignment.view(-1, max_sample, self.cluster_size)  # -> B x N x K
        a_sum = th.sum(assignment, dim=1, keepdim=True)  # B x N x K -> B x 1 x K
        a = a_sum * self.clusters2

        assignment = assignment.transpose(1, 2)  # B x N x K -> B x K x N

        x = x.view(-1, max_sample, self.feature_size)  # BN x D -> B x N x D
        vlad = th.matmul(assignment, x)  # (B x K x N) x (B x N x D) -> B x K x D
        vlad = vlad.transpose(1, 2)  # -> B x D x K
        vlad = vlad - a

        # L2 intra norm
        vlad = F.normalize(vlad)

        # flattening + L2 norm
        vlad = vlad.reshape(-1, self.cluster_size * self.feature_size)  # -> B x DK
        vlad = F.normalize(vlad)
        return vlad  # B x DK

batch_size = 1024
num_features = 50
num_clusters = 16
feature_size = 256
ghost_clusters = 0

def get_inputs():
  return [torch.rand(batch_size, num_features, feature_size)]

def get_init_inputs():
  return [num_clusters, feature_size, ghost_clusters]

CUDA candidate (to audit):

# Copyright 2018 Antoine Miech All Rights Reserved.
# Optimized with custom CUDA kernels for soft-max and L2-normalisation

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

##############################################################################
#                           CUDA KERNELS                                     #
##############################################################################

cuda_src = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <float.h>

// ---------------------------------------------
// helpers : row-wise reductions inside a block
// ---------------------------------------------
template<int BLOCK_SIZE>
__device__ float block_reduce_max(float v){
    __shared__ float smem[BLOCK_SIZE];
    smem[threadIdx.x] = v;
    __syncthreads();
    for(int stride = BLOCK_SIZE/2; stride>0; stride >>= 1){
        if(threadIdx.x < stride){
            smem[threadIdx.x] = fmaxf(smem[threadIdx.x], smem[threadIdx.x + stride]);
        }
        __syncthreads();
    }
    return smem[0];
}

template<int BLOCK_SIZE>
__device__ float block_reduce_sum(float v){
    __shared__ float smem[BLOCK_SIZE];
    smem[threadIdx.x] = v;
    __syncthreads();
    for(int stride = BLOCK_SIZE/2; stride>0; stride >>= 1){
        if(threadIdx.x < stride){
            smem[threadIdx.x] += smem[threadIdx.x + stride];
        }
        __syncthreads();
    }
    return smem[0];
}

// ------------------------------------------------------
// Row-wise soft-max  (2-D tensor, contiguous) â€‘ forward
// ------------------------------------------------------
template<int BLOCK_SIZE>
__global__ void rowwise_softmax_kernel(const float* __restrict__ in,
                                       float* __restrict__ out,
                                       const int rows,
                                       const int cols){
    int row = blockIdx.x;
    if(row >= rows) return;

    // compute max per row for numerical stability
    float local_max = -FLT_MAX;
    for(int idx = threadIdx.x; idx < cols; idx += BLOCK_SIZE){
        local_max = fmaxf(local_max, in[row*cols + idx]);
    }
    float row_max = block_reduce_max<BLOCK_SIZE>(local_max);

    // exponentiate & accumulate
    float local_sum = 0.f;
    for(int idx = threadIdx.x; idx < cols; idx += BLOCK_SIZE){
        float val = expf(in[row*cols + idx] - row_max);
        out[row*cols + idx] = val;      // store numerator for now
        local_sum += val;
    }
    float row_sum = block_reduce_sum<BLOCK_SIZE>(local_sum) + 1e-6f; // avoid div by 0

    // normalise
    for(int idx = threadIdx.x; idx < cols; idx += BLOCK_SIZE){
        out[row*cols + idx] /= row_sum;
    }
}

// ------------------------------------------------------
// Row-wise L2 normalisation  (2-D tensor, contiguous)
// ------------------------------------------------------
template<int BLOCK_SIZE>
__global__ void rowwise_l2norm_kernel(const float* __restrict__ in,
                                      float* __restrict__ out,
                                      const int rows,
                                      const int cols,
                                      const float eps){
    int row = blockIdx.x;
    if(row >= rows) return;

    // accumulate sum of squares
    float local_sum = 0.f;
    for(int idx = threadIdx.x; idx < cols; idx += BLOCK_SIZE){
        float v = in[row*cols + idx];
        local_sum += v*v;
    }
    float row_sum = block_reduce_sum<BLOCK_SIZE>(local_sum);
    float inv_norm = rsqrtf(row_sum + eps);

    // write normalised values
    for(int idx = threadIdx.x; idx < cols; idx += BLOCK_SIZE){
        out[row*cols + idx] = in[row*cols + idx] * inv_norm;
    }
}

// ------------------------------------------------------
// C++/Python bindings
// ------------------------------------------------------
torch::Tensor softmax_rowwise_cuda(torch::Tensor input){
    TORCH_CHECK(input.is_cuda(), "Input must reside on GPU");
    TORCH_CHECK(input.dim()==2 && input.is_contiguous(), "Expect contiguous 2-D tensor");

    auto rows = input.size(0);
    auto cols = input.size(1);
    auto output = torch::empty_like(input);

    constexpr int BLOCK = 256;
    rowwise_softmax_kernel<BLOCK><<<rows, BLOCK>>>(input.data_ptr<float>(),
                                                   output.data_ptr<float>(),
                                                   rows, cols);
    return output;
}

torch::Tensor l2norm_rowwise_cuda(torch::Tensor input, double eps){
    TORCH_CHECK(input.is_cuda(), "Input must reside on GPU");
    TORCH_CHECK(input.dim()==2 && input.is_contiguous(), "Expect contiguous 2-D tensor");

    auto rows = input.size(0);
    auto cols = input.size(1);
    auto output = torch::empty_like(input);

    constexpr int BLOCK = 256;
    rowwise_l2norm_kernel<BLOCK><<<rows, BLOCK>>>(input.data_ptr<float>(),
                                                  output.data_ptr<float>(),
                                                  rows, cols,
                                                  static_cast<float>(eps));
    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){
    m.def("softmax_rowwise_cuda", &softmax_rowwise_cuda, "Row-wise softmax (CUDA)");
    m.def("l2norm_rowwise_cuda", &l2norm_rowwise_cuda, "Row-wise L2 norm (CUDA)");
}
"""

cpp_decls = """
torch::Tensor softmax_rowwise_cuda(torch::Tensor input);
torch::Tensor l2norm_rowwise_cuda(torch::Tensor input, double eps);
"""

ops = load_inline(
    name="vlad_custom_ops",
    cpp_sources=cpp_decls,
    cuda_sources=cuda_src,
    functions=["softmax_rowwise_cuda", "l2norm_rowwise_cuda"],
    verbose=False,
)

##############################################################################
#                        Optimised Model (ModelNew)                          #
##############################################################################

class ModelNew(nn.Module):
    def __init__(self, cluster_size, feature_size, ghost_clusters):
        super().__init__()

        self.feature_size = feature_size
        self.cluster_size = cluster_size
        self.ghost_clusters = ghost_clusters

        init_sc = 1.0 / math.sqrt(feature_size)
        clusters = cluster_size + ghost_clusters

        self.clusters = nn.Parameter(init_sc * torch.randn(feature_size, clusters))
        self.batch_norm = nn.BatchNorm1d(clusters)
        self.clusters2 = nn.Parameter(init_sc * torch.randn(1, feature_size, cluster_size))
        self.out_dim = self.cluster_size * feature_size

        # expose kernels
        self.softmax_cuda = ops.softmax_rowwise_cuda
        self.l2norm_cuda = ops.l2norm_rowwise_cuda

    def forward(self, x, mask=None):
        """
        Args:
            x (Tensor): Shape B x N x D
        Returns:
            Tensor: Shape B x (K*D)
        """
        B, N, D = x.shape
        x_flat = x.reshape(-1, self.feature_size)               # BN x D

        assignment = torch.matmul(x_flat, self.clusters)        # BN x (K+G)
        assignment = self.batch_norm(assignment)

        # --- custom softmax (row-wise on BN x (K+G)) ---
        assignment = self.softmax_cuda(assignment.contiguous()) # BN x (K+G)

        assignment = assignment[:, :self.cluster_size]          # drop ghost
        assignment = assignment.view(B, N, self.cluster_size)   # B x N x K
        a_sum = assignment.sum(dim=1, keepdim=True)             # B x 1 x K
        a = a_sum * self.clusters2                              # B x D x K

        assignment = assignment.transpose(1, 2)                 # B x K x N
        x_reshaped = x.reshape(B, N, D)                         # B x N x D

        vlad = torch.matmul(assignment, x_reshaped)             # B x K x D
        vlad = vlad.transpose(1, 2)                             # B x D x K
        vlad = vlad - a

        # intra-cluster normalisation (uses native for 3-D tensor)
        vlad = F.normalize(vlad, p=2, dim=1)

        # flatten
        vlad = vlad.reshape(B, -1)                              # B x (K*D)

        # --- custom L2 row-wise normalisation (over final feature dim) ---
        vlad = self.l2norm_cuda(vlad.contiguous(), 1e-12)

        return vlad

##############################################################################
#                       Helpers for bench / example                          #
##############################################################################
batch_size     = 1024
num_features   = 50
num_clusters   = 16
feature_size   = 256
ghost_clusters = 0

def get_inputs():
    return [torch.rand(batch_size, num_features, feature_size, device="cuda")]

def get_init_inputs():
    return [num_clusters, feature_size, ghost_clusters]


Follow the Rules and produce the JSON exactly in the specified format.