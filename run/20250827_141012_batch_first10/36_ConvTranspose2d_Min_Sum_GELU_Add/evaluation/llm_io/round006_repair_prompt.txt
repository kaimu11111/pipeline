You are a senior CUDA-extension developer.
Your job is to **FIX** the compilation or runtime errors in the Python script
shown below.

OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
2. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

────────────────────────────────────────
ERROR LOG
────────────────────────────────────────
Traceback (most recent call last):
  File "/home/wan00559/pipleline/utils/compile_and_run.py", line 516, in compare_and_bench
    test_model = ModelNew(*init_args, **init_kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wan00559/pipleline/run/20250827_141012_batch_first10/36_ConvTranspose2d_Min_Sum_GELU_Add/code/kernel_20250827_143920.py", line 154, in __init__
    self.conv_transpose = nn.ConvTranspose2d(*raw_args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 1127, in __init__
    super().__init__(
  File "/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 751, in __init__
    super().__init__(
  File "/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 104, in __init__
    if groups <= 0:
       ^^^^^^^^^^^
TypeError: '<=' not supported between instances of 'tuple' and 'int'

────────────────────────────────────────
OLD CODE (read-only)
────────────────────────────────────────
# 1. ──────────────────────────────────────────────────────────────────────
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# 2. ──────────────────────────────────────────────────────────────────────
source = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

////////////////////////////////////////////////////////////////////////////////
// fast-GELU (tanh approximation, same as torch.nn.functional.gelu(...,approx="tanh"))
////////////////////////////////////////////////////////////////////////////////
__device__ __forceinline__ float gelu(float x) {
    const float kAlpha = 0.7978845608f;          // sqrt(2/pi)
    const float kBeta  = 0.044715f;
    return 0.5f * x * (1.f + tanhf(kAlpha * (x + kBeta * x * x * x)));
}

////////////////////////////////////////////////////////////////////////////////
// kernel:  (N,C,H,W) ─► (N,1,1,W)
//          1) min along C
//          2) sum along H
//          3) GELU
//          4) add scalar bias
////////////////////////////////////////////////////////////////////////////////
__global__ void fused_kernel(
        const float* __restrict__ x,
        const float* __restrict__ bias,   // scalar bias (length==1)
        float*       __restrict__ out,
        int N, int C, int H, int W)
{
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int nW  = N * W;
    if (idx >= nW) return;

    const int n = idx / W;      // batch
    const int w = idx % W;      // width

    float sum_h = 0.f;

    for (int h = 0; h < H; ++h) {
        // channel 0
        int off   = (((n * C + 0) * H + h) * W) + w;
        float vmin = x[off];
        // remaining channels
        for (int c = 1; c < C; ++c) {
            off += H * W;                 // advance 1 channel
            float v = x[off];
            if (v < vmin) vmin = v;
        }
        sum_h += vmin;
    }

    const float b = bias[0];              // scalar bias
    out[n * W + w] = gelu(sum_h) + b;
}

////////////////////////////////////////////////////////////////////////////////
// C++ launcher
////////////////////////////////////////////////////////////////////////////////
torch::Tensor fused_min_sum_gelu_bias_cuda(torch::Tensor x,
                                           torch::Tensor bias) {
    TORCH_CHECK(x.is_cuda() && bias.is_cuda(), "tensors must be CUDA");
    TORCH_CHECK(x.dtype() == torch::kFloat32 && bias.dtype() == torch::kFloat32,
                "tensors must be float32");
    TORCH_CHECK(bias.numel() == 1,
                "bias tensor must contain exactly one element (scalar)");

    const int N = x.size(0);
    const int C = x.size(1);
    const int H = x.size(2);
    const int W = x.size(3);

    auto out = torch::empty({N,1,1,W}, x.options());

    const int threads = 256;
    const int blocks  = (N * W + threads - 1) / threads;

    fused_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W);

    cudaError_t err = cudaGetLastError();
    TORCH_CHECK(err == cudaSuccess, cudaGetErrorString(err));

    return out;
}
"""

# 3. ──────────────────────────────────────────────────────────────────────
cpp_src = r"""
torch::Tensor fused_min_sum_gelu_bias_cuda(torch::Tensor x,
                                           torch::Tensor bias);
"""

# 4. ──────────────────────────────────────────────────────────────────────
fused_ops = load_inline(
    name         = "fused_min_sum_gelu_bias",
    cpp_sources  = cpp_src,
    cuda_sources = source,
    functions    = ["fused_min_sum_gelu_bias_cuda"],
    verbose      = False,
)

# 5. ──────────────────────────────────────────────────────────────────────
class ModelNew(nn.Module):
    """
    Optimised replacement for the reference model (scalar fused bias).
    Supports the same construction patterns as the original implementation.
    """
    def __init__(self, *args, **kwargs):
        super().__init__()

        fused_bias = kwargs.pop("fused_bias", None)

        # ── branch 1 : first arg is nn.ConvTranspose2d ───────────────────
        if len(args) >= 1 and isinstance(args[0], nn.ConvTranspose2d):
            ref_conv = args[0]

            # optional fused bias tensor
            if len(args) >= 2 and isinstance(args[1], torch.Tensor) and args[1].numel() == 1:
                fused_bias = args[1]

            # clone conv so hyper-params & weights match
            self.conv_transpose = nn.ConvTranspose2d(
                in_channels    = ref_conv.in_channels,
                out_channels   = ref_conv.out_channels,
                kernel_size    = ref_conv.kernel_size,
                stride         = ref_conv.stride,
                padding        = ref_conv.padding,
                output_padding = ref_conv.output_padding,
                groups         = ref_conv.groups,
                bias           = ref_conv.bias is not None,
                dilation       = ref_conv.dilation,
                padding_mode   = ref_conv.padding_mode,
            )
            self.conv_transpose.load_state_dict(ref_conv.state_dict())

        # ── branch 2 : raw ConvTranspose2d construction args ─────────────
        else:
            raw_args = list(args)

            # trailing tensor → fused bias tensor
            if len(raw_args) >= 1 and isinstance(raw_args[-1], torch.Tensor) and raw_args[-1].numel() == 1:
                fused_bias = raw_args.pop()

            if len(raw_args) == 0:
                raise ValueError("ConvTranspose2d construction arguments missing.")
            self.conv_transpose = nn.ConvTranspose2d(*raw_args, **kwargs)

        # register scalar fused bias parameter
        if fused_bias is not None:
            if fused_bias.numel() != 1:
                raise ValueError("Fused bias must be a scalar (numel==1).")
            self.register_parameter("bias", nn.Parameter(fused_bias.clone().detach()))
        else:
            # initialise scalar bias to zero
            self.register_parameter("bias", nn.Parameter(torch.zeros(1)))

    # ────────────────────────────────────────────────────────────────────
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv_transpose(x)
        x = x.contiguous()
        return fused_ops.fused_min_sum_gelu_bias_cuda(x, self.bias)

────────────────────────────────────────
Main Critical Problem
────────────────────────────────────────
critical_issue: bias_shape tuple passed as groups param
why_it_matters: ModelNew mis-parses args; ConvTranspose2d gets groups=(1,1,1), triggers TypeError, model never runs.
minimal_fix_hint: remove/pop bias_shape tuple before conv init

```python
# <your corrected code>
```
# ==========================================================
