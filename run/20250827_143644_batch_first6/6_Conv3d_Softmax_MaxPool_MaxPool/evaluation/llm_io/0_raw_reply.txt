```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ----------------------------------------------------------------------
# CUDA implementation of channelâ€“wise soft-max for 5-D NCDHW tensors
# ----------------------------------------------------------------------
softmax_cuda_src = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cfloat>       // for FLT_MAX
#include <math.h>

////////////////////////////////////////////////////////////////////////////////
// CUDA kernel
////////////////////////////////////////////////////////////////////////////////
__global__ void softmax_channel_kernel(const float* __restrict__ in,
                                       float* __restrict__ out,
                                       const int C,
                                       const int DHW,
                                       const int total_elements) {
    int linear_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (linear_idx >= total_elements) return;

    // Offset of the first channel for this (n, d, h, w) position
    int base_offset = linear_idx;          // offset within one channel
    const float* in_ptr  = in  + base_offset;
    float*       out_ptr = out + base_offset;

    // Step between successive channels (because layout is N C D H W, contiguous)
    const int stride = DHW;

    // 1) Find maximum value across C to improve numerical stability
    float max_val = -FLT_MAX;
    for (int c = 0; c < C; ++c) {
        float v = in_ptr[c * stride];
        max_val = v > max_val ? v : max_val;
    }

    // 2) Compute sum of exp(.) and store intermediate exp values to output
    float sum_exp = 0.0f;
    for (int c = 0; c < C; ++c) {
        float e = expf(in_ptr[c * stride] - max_val);
        out_ptr[c * stride] = e;           // Temporarily keep numerator
        sum_exp += e;
    }
    float inv_sum = 1.0f / sum_exp;

    // 3) Normalize
    for (int c = 0; c < C; ++c) {
        out_ptr[c * stride] *= inv_sum;
    }
}

////////////////////////////////////////////////////////////////////////////////
// C++ interface
////////////////////////////////////////////////////////////////////////////////
torch::Tensor softmax_channel_cuda(torch::Tensor x) {
    TORCH_CHECK(x.is_cuda(), "Tensor must reside on CUDA device");
    TORCH_CHECK(x.scalar_type() == torch::kFloat32,
                "Only float32 tensors are supported");

    auto y = torch::empty_like(x);

    const int N  = x.size(0);
    const int C  = x.size(1);
    const int D  = x.size(2);
    const int H  = x.size(3);
    const int W  = x.size(4);
    const int DHW   = D * H * W;
    const int total = N * DHW;

    const int block_size = 256;
    const int grid_size  = (total + block_size - 1) / block_size;

    softmax_channel_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        C,
        DHW,
        total);

    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
        throw std::runtime_error(cudaGetErrorString(err));

    return y;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("softmax_channel_cuda", &softmax_channel_cuda,
          "Channel-wise softmax for 5-D tensors (CUDA)");
}
"""

# Exposed prototype for the function we just defined
cpp_decl = "torch::Tensor softmax_channel_cuda(torch::Tensor x);"

# Build the extension
softmax_cuda = load_inline(
    name="softmax_channel",
    cpp_sources=cpp_decl,
    cuda_sources=softmax_cuda_src,
    functions=["softmax_channel_cuda"],
    verbose=False,
)

# ----------------------------------------------------------------------
# Optimised model using the custom CUDA soft-max
# ----------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Model that performs a 3D convolution, uses a custom CUDA
    soft-max along the channel dimension, and applies two max-pool layers.
    """
    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super().__init__()
        self.conv   = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.pool1  = nn.MaxPool3d(pool_kernel_size)
        self.pool2  = nn.MaxPool3d(pool_kernel_size)

    def forward(self, x):
        x = self.conv(x)
        # Custom CUDA soft-max (channel-wise)
        x = softmax_cuda.softmax_channel_cuda(x.contiguous())
        x = self.pool1(x)
        x = self.pool2(x)
        return x
```