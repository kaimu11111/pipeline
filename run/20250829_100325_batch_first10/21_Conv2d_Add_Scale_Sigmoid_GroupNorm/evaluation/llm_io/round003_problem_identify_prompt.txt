You are given:

ERROR_LOG:
/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Error building extension 'fused_bias_scale_sigmoid_fast': [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=fused_bias_scale_sigmoid_fast -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_bias_scale_sigmoid_fast/main.cpp -o main.o 
[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=fused_bias_scale_sigmoid_fast -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_bias_scale_sigmoid_fast/cuda.cu -o cuda.cuda.o 
FAILED: cuda.cuda.o 
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=fused_bias_scale_sigmoid_fast -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_bias_scale_sigmoid_fast/cuda.cu -o cuda.cuda.o 
/home/wan00559/.cache/torch_extensions/py311_cu126/fused_bias_scale_sigmoid_fast/cuda.cu(67): error: namespace "at::cuda" has no member "getCurrentCUDAStream"

1 error detected in the compilation of "/home/wan00559/.cache/torch_extensions/py311_cu126/fused_bias_scale_sigmoid_fast/cuda.cu".
ninja: build stopped: subcommand failed.

PyTorch reference (ground truth):

import torch
import torch.nn as nn

class Model(nn.Module):
    """
    Model that performs a convolution, adds a bias term, scales, applies sigmoid, and performs group normalization.
    """
    def __init__(self, in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape)) 
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x):
        x = self.conv(x)
        x = x + self.bias
        x = x * self.scale
        x = torch.sigmoid(x)
        x = self.group_norm(x)
        return x

batch_size = 64
in_channels = 4
out_channels = 16
height = width = 128
kernel_size = 3
num_groups = 8
bias_shape = (out_channels, 1, 1)
scale_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width)]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, num_groups, bias_shape, scale_shape]

CUDA candidate (to audit):

# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline


###############################################################################
# CUDA kernel with improved ILP and reduced launch count
###############################################################################
source = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename T>
__device__ __forceinline__ T sigmoid(T x);

template <>
__device__ __forceinline__ float sigmoid<float>(float x) {
    return 1.f / (1.f + expf(-x));
}

/*  Each CUDA thread processes `ILP` contiguous elements to increase
    instruction-level parallelism and memory-throughput utilisation.    */
constexpr int ILP = 4;

template <typename scalar_t>
__global__ void fused_bias_scale_sigmoid_kernel(
        const scalar_t* __restrict__ x,
        const scalar_t* __restrict__ bias,
        const scalar_t* __restrict__ scale,
        scalar_t*       __restrict__ out,
        int C,
        int hw,          // H*W
        int numel) {

    const int idx0 = (blockIdx.x * blockDim.x + threadIdx.x) * ILP;

    #pragma unroll
    for (int k = 0; k < ILP; ++k) {
        int idx = idx0 + k;
        if (idx >= numel) break;

        int c = (idx / hw) % C;              // channel index (NCHW)
        scalar_t v = (x[idx] + bias[c]) * scale[c];
        out[idx]  = sigmoid<scalar_t>(v);
    }
}

torch::Tensor fused_bias_scale_sigmoid_cuda(
        torch::Tensor x,
        torch::Tensor bias,
        torch::Tensor scale) {

    TORCH_CHECK(x.is_cuda(),    "x must be a CUDA tensor");
    TORCH_CHECK(bias.is_cuda(), "bias must be a CUDA tensor");
    TORCH_CHECK(scale.is_cuda(),"scale must be a CUDA tensor");
    TORCH_CHECK(x.dtype() == torch::kFloat32,
                "only float32 is currently supported");

    const int  C      = x.size(1);
    const int  H      = x.size(2);
    const int  W      = x.size(3);
    const int  hw     = H * W;
    const long numel  = x.numel();

    auto out = torch::empty_like(x);

    constexpr int threads = 256;
    const long blocks = (numel + threads * ILP - 1) / (threads * ILP);

    fused_bias_scale_sigmoid_kernel<float><<<blocks, threads, 0,
        at::cuda::getCurrentCUDAStream()>>>(
            x.data_ptr<float>(),
            bias.data_ptr<float>(),
            scale.data_ptr<float>(),
            out.data_ptr<float>(),
            C,
            hw,
            numel);

    return out;
}
"""

###############################################################################
# C++ prototypes exposed to Python
###############################################################################
cpp_src = r"""
torch::Tensor fused_bias_scale_sigmoid_cuda(torch::Tensor x,
                                            torch::Tensor bias,
                                            torch::Tensor scale);
"""

###############################################################################
# Compile & load the extension
###############################################################################
fused_bias_scale_sigmoid = load_inline(
    name        = "fused_bias_scale_sigmoid_fast",
    cpp_sources = cpp_src,
    cuda_sources= source,
    functions   = ["fused_bias_scale_sigmoid_cuda"],
    verbose     = False,
)


###############################################################################
# PyTorch module invoking the custom kernel
###############################################################################
class ModelNew(nn.Module):
    """
    Optimised model:
      Conv2d (no padding) -> fused (bias + scale + sigmoid) CUDA kernel
      -> GroupNorm
    """
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size,
                 num_groups,
                 bias_shape,
                 scale_shape):
        super().__init__()
        # Keep behaviour identical to reference implementation
        self.conv = nn.Conv2d(in_channels,
                              out_channels,
                              kernel_size,
                              padding=0)

        self.bias  = nn.Parameter(torch.randn(bias_shape))
        self.scale = nn.Parameter(torch.randn(scale_shape))
        self.group_norm = nn.GroupNorm(num_groups, out_channels)

    def forward(self, x):
        x = self.conv(x)
        x = fused_bias_scale_sigmoid.fused_bias_scale_sigmoid_cuda(
            x, self.bias, self.scale)
        x = self.group_norm(x)
        return x


###############################################################################
# Helper functions expected by the benchmarking harness
###############################################################################
batch_size  = 64
in_channels = 4
out_channels= 16
height = width = 128
kernel_size = 3
num_groups  = 8
bias_shape  = (out_channels, 1, 1)
scale_shape = (out_channels, 1, 1)

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width, device="cuda")]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, num_groups,
            bias_shape, scale_shape]
# </complete ModelNew code>


Follow the Rules and produce the JSON exactly in the specified format.