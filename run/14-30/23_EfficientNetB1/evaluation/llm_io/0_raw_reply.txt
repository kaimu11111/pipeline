```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------------
# Custom CUDA implementation of ReLU6 (forward + backward)
# ---------------------------------------------------------------------------

cuda_source = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void relu6_forward_kernel(const scalar_t* __restrict__ input,
                                     scalar_t* __restrict__ output,
                                     int64_t numel) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= numel) return;
    scalar_t val = input[idx];
    val = val < scalar_t(0) ? scalar_t(0) : (val > scalar_t(6) ? scalar_t(6) : val);
    output[idx] = val;
}

template <typename scalar_t>
__global__ void relu6_backward_kernel(const scalar_t* __restrict__ input,
                                      const scalar_t* __restrict__ grad_output,
                                      scalar_t* __restrict__ grad_input,
                                      int64_t numel) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= numel) return;
    scalar_t in_val = input[idx];
    scalar_t go     = grad_output[idx];
    grad_input[idx] = (in_val > scalar_t(0) && in_val < scalar_t(6)) ? go : scalar_t(0);
}

torch::Tensor relu6_forward_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    int64_t numel = input.numel();
    const int threads = 256;
    const int blocks  = (numel + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), "relu6_forward_cuda", ([&] {
        relu6_forward_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            numel);
    }));
    return output;
}

torch::Tensor relu6_backward_cuda(torch::Tensor input, torch::Tensor grad_output) {
    auto grad_input = torch::empty_like(input);
    int64_t numel = input.numel();
    const int threads = 256;
    const int blocks  = (numel + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), "relu6_backward_cuda", ([&] {
        relu6_backward_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            grad_output.data_ptr<scalar_t>(),
            grad_input.data_ptr<scalar_t>(),
            numel);
    }));
    return grad_input;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("relu6_forward_cuda",  &relu6_forward_cuda,  "ReLU6 forward (CUDA)");
    m.def("relu6_backward_cuda", &relu6_backward_cuda, "ReLU6 backward (CUDA)");
}
"""

cpp_decl = """
torch::Tensor relu6_forward_cuda(torch::Tensor input);
torch::Tensor relu6_backward_cuda(torch::Tensor input, torch::Tensor grad_output);
"""

relu6_cuda = load_inline(
    name        = "custom_relu6",
    cpp_sources = cpp_decl,
    cuda_sources = cuda_source,
    functions   = ["relu6_forward_cuda", "relu6_backward_cuda"],
    verbose     = False,
)

# ---------------------------------------------------------------------------
# Python autograd wrapper around the CUDA ReLU6 kernel
# ---------------------------------------------------------------------------

class _ReLU6Function(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return relu6_cuda.relu6_forward_cuda(input)

    @staticmethod
    def backward(ctx, grad_output):
        (input,) = ctx.saved_tensors
        return relu6_cuda.relu6_backward_cuda(input, grad_output)

class CustomReLU6(nn.Module):
    def forward(self, x):
        return _ReLU6Function.apply(x)

# ---------------------------------------------------------------------------
# Optimised EfficientNet-B1 model that uses the custom ReLU6
# ---------------------------------------------------------------------------

class ModelNew(nn.Module):
    def __init__(self, num_classes=1000):
        super().__init__()

        # Initial convolutional stem
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1   = nn.BatchNorm2d(32)

        # MBConv stages
        self.mbconv1 = self._make_mbconv_block( 32,  16, stride=1, expand_ratio=1)
        self.mbconv2 = self._make_mbconv_block( 16,  24, stride=2, expand_ratio=6)
        self.mbconv3 = self._make_mbconv_block( 24,  40, stride=2, expand_ratio=6)
        self.mbconv4 = self._make_mbconv_block( 40,  80, stride=2, expand_ratio=6)
        self.mbconv5 = self._make_mbconv_block( 80, 112, stride=1, expand_ratio=6)
        self.mbconv6 = self._make_mbconv_block(112, 192, stride=2, expand_ratio=6)
        self.mbconv7 = self._make_mbconv_block(192, 320, stride=1, expand_ratio=6)

        # Head
        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2   = nn.BatchNorm2d(1280)

        # Classification layer
        self.fc    = nn.Linear(1280, num_classes)

    def _make_mbconv_block(self, in_ch, out_ch, stride, expand_ratio):
        hidden_dim = round(in_ch * expand_ratio)
        return nn.Sequential(
            nn.Conv2d(in_ch, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(hidden_dim),
            CustomReLU6(),

            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1,
                      groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            CustomReLU6(),

            nn.Conv2d(hidden_dim, out_ch, kernel_size=1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(out_ch),
        )

    def forward(self, x):
        # Stem
        x = F.relu(self.bn1(self.conv1(x)))

        # MBConv stages
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.mbconv6(x)
        x = self.mbconv7(x)

        # Head
        x = F.relu(self.bn2(self.conv2(x)))

        # Pool & FC
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x
```