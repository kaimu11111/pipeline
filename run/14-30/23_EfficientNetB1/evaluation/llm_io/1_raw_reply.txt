```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------------
# CUDA kernels + host wrappers (no PYBIND11_MODULE here)
# ---------------------------------------------------------------------------
source = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void relu6_forward_kernel(const scalar_t* __restrict__ input,
                                     scalar_t* __restrict__ output,
                                     int64_t numel) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= numel) return;
    scalar_t val = input[idx];
    val = val < scalar_t(0) ? scalar_t(0)
         : (val > scalar_t(6) ? scalar_t(6) : val);
    output[idx] = val;
}

template <typename scalar_t>
__global__ void relu6_backward_kernel(const scalar_t* __restrict__ input,
                                      const scalar_t* __restrict__ grad_output,
                                      scalar_t* __restrict__ grad_input,
                                      int64_t numel) {
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= numel) return;
    const scalar_t in_val = input[idx];
    const scalar_t go     = grad_output[idx];
    grad_input[idx] = (in_val > scalar_t(0) && in_val < scalar_t(6)) ? go
                                                                    : scalar_t(0);
}

torch::Tensor relu6_forward_cuda(torch::Tensor input) {
    auto output = torch::empty_like(input);
    const int64_t numel = input.numel();
    constexpr int threads = 256;
    const int blocks = (numel + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), "relu6_forward_cuda", ([&] {
        relu6_forward_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>(),
            numel);
    }));
    return output;
}

torch::Tensor relu6_backward_cuda(torch::Tensor input, torch::Tensor grad_output) {
    auto grad_input = torch::empty_like(input);
    const int64_t numel = input.numel();
    constexpr int threads = 256;
    const int blocks = (numel + threads - 1) / threads;

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), "relu6_backward_cuda", ([&] {
        relu6_backward_kernel<scalar_t><<<blocks, threads>>>(
            input.data_ptr<scalar_t>(),
            grad_output.data_ptr<scalar_t>(),
            grad_input.data_ptr<scalar_t>(),
            numel);
    }));
    return grad_input;
}
"""

# ---------------------------------------------------------------------------
# Function prototypes for binding generation
# ---------------------------------------------------------------------------
cpp_src = """
torch::Tensor relu6_forward_cuda(torch::Tensor input);
torch::Tensor relu6_backward_cuda(torch::Tensor input, torch::Tensor grad_output);
"""

# ---------------------------------------------------------------------------
# Build and load the extension (load_inline adds the PYBIND11_MODULE wrapper)
# ---------------------------------------------------------------------------
relu6_cuda = load_inline(
    name         = "custom_relu6",
    cpp_sources  = cpp_src,
    cuda_sources = source,
    functions    = ["relu6_forward_cuda", "relu6_backward_cuda"],
    verbose      = False,
)

# Alias for functional namespace (keeps original call sites unchanged)
F = torch.nn.functional

# ---------------------------------------------------------------------------
# Autograd bridge utilising the compiled kernels
# ---------------------------------------------------------------------------
class _ReLU6Function(torch.autograd.Function):
    @staticmethod
    def forward(ctx, inp):
        ctx.save_for_backward(inp)
        return relu6_cuda.relu6_forward_cuda(inp)

    @staticmethod
    def backward(ctx, grad_out):
        (inp,) = ctx.saved_tensors
        return relu6_cuda.relu6_backward_cuda(inp, grad_out)

class CustomReLU6(nn.Module):
    def forward(self, x):
        return _ReLU6Function.apply(x)

# ---------------------------------------------------------------------------
# EfficientNet-B1 skeleton using the custom ReLU6 kernels
# ---------------------------------------------------------------------------
class ModelNew(nn.Module):
    def __init__(self, num_classes: int = 1000):
        super().__init__()

        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2,
                               padding=1, bias=False)
        self.bn1   = nn.BatchNorm2d(32)

        self.mbconv1 = self._make_mbconv_block( 32,  16, stride=1, expand_ratio=1)
        self.mbconv2 = self._make_mbconv_block( 16,  24, stride=2, expand_ratio=6)
        self.mbconv3 = self._make_mbconv_block( 24,  40, stride=2, expand_ratio=6)
        self.mbconv4 = self._make_mbconv_block( 40,  80, stride=2, expand_ratio=6)
        self.mbconv5 = self._make_mbconv_block( 80, 112, stride=1, expand_ratio=6)
        self.mbconv6 = self._make_mbconv_block(112, 192, stride=2, expand_ratio=6)
        self.mbconv7 = self._make_mbconv_block(192, 320, stride=1, expand_ratio=6)

        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, bias=False)
        self.bn2   = nn.BatchNorm2d(1280)

        self.fc    = nn.Linear(1280, num_classes)

    def _make_mbconv_block(self, in_ch, out_ch, stride, expand_ratio):
        hidden_dim = round(in_ch * expand_ratio)
        return nn.Sequential(
            nn.Conv2d(in_ch, hidden_dim, kernel_size=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            CustomReLU6(),

            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride,
                      padding=1, groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            CustomReLU6(),

            nn.Conv2d(hidden_dim, out_ch, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_ch),
        )

    def forward(self, x):
        # Stem
        x = F.relu(self.bn1(self.conv1(x)))

        # MBConv stages
        x = self.mbconv1(x)
        x = self.mbconv2(x)
        x = self.mbconv3(x)
        x = self.mbconv4(x)
        x = self.mbconv5(x)
        x = self.mbconv6(x)
        x = self.mbconv7(x)

        # Head
        x = F.relu(self.bn2(self.conv2(x)))

        # Pool & FC
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        return self.fc(x)
```