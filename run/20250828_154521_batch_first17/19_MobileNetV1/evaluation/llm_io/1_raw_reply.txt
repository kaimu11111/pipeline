import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------------
# 1. CUDA source (kernel + host wrapper)
# ---------------------------------------------------------------------------
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

// ---------------------------------------------------------------------------
// Kernel
// ---------------------------------------------------------------------------
__global__ void bn_relu_kernel(
        const float* __restrict__ x,
        const float* __restrict__ scale,
        const float* __restrict__ shift,
        float* __restrict__ y,
        const int  C,
        const int  spatial,
        const long long numel) {

    const long long idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= numel) return;

    const int c   = (idx / spatial) % C;
    float val     = x[idx] * scale[c] + shift[c];
    y[idx]        = val > 0.f ? val : 0.f;
}

// ---------------------------------------------------------------------------
// Host wrapper
// ---------------------------------------------------------------------------
torch::Tensor fused_bn_relu_cuda(torch::Tensor x,
                                 torch::Tensor scale,
                                 torch::Tensor shift) {
    TORCH_CHECK(x.is_cuda(),                "x must be a CUDA tensor");
    TORCH_CHECK(scale.is_cuda() && shift.is_cuda(),
                "scale and shift must be CUDA tensors");
    TORCH_CHECK(x.scalar_type() == torch::kFloat32,
                "only float32 tensors are supported");

    const int  N = x.size(0);
    const int  C = x.size(1);
    int spatial = 1;
    for (int i = 2; i < x.dim(); ++i) spatial *= x.size(i);
    const long long numel = static_cast<long long>(N) * C * spatial;

    auto y = torch::empty_like(x);

    const int block = 256;
    const int grid  = static_cast<int>((numel + block - 1) / block);

    bn_relu_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        scale.data_ptr<float>(),
        shift.data_ptr<float>(),
        y.data_ptr<float>(),
        C,
        spatial,
        numel);

    cudaError_t err = cudaGetLastError();
    TORCH_CHECK(err == cudaSuccess,
                "CUDA kernel launch failed (bn_relu_kernel): ",
                cudaGetErrorString(err));

    return y;
}
"""

# ---------------------------------------------------------------------------
# 2. Function prototype(s)
# ---------------------------------------------------------------------------
cpp_src = "torch::Tensor fused_bn_relu_cuda(torch::Tensor x, torch::Tensor scale, torch::Tensor shift);"

# ---------------------------------------------------------------------------
# 3. Compile & load the extension
# ---------------------------------------------------------------------------
fused_bn_relu = load_inline(
    name        = "fused_bn_relu",
    cpp_sources = cpp_src,
    cuda_sources= source,
    functions   = ["fused_bn_relu_cuda"],
    verbose     = False,
)

# ---------------------------------------------------------------------------
# 4. PyTorch modules
# ---------------------------------------------------------------------------
class FusedBNReLU(nn.Module):
    """
    Replaces BatchNorm + ReLU during evaluation with a custom fused CUDA op.
    Falls back to the native ops during training so statistics update is correct.
    """
    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1):
        super().__init__()
        self.eps      = eps
        self.momentum = momentum
        self.weight   = nn.Parameter(torch.ones(num_features))
        self.bias     = nn.Parameter(torch.zeros(num_features))
        self.register_buffer("running_mean", torch.zeros(num_features))
        self.register_buffer("running_var",  torch.ones(num_features))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.training:
            x = nn.functional.batch_norm(
                x, self.running_mean, self.running_var,
                self.weight, self.bias,
                True, self.momentum, self.eps,
            )
            return nn.functional.relu(x, inplace=True)

        # Inference path â€“ fuse BN affine transform with ReLU
        scale = self.weight / torch.sqrt(self.running_var + self.eps)
        shift = self.bias - self.running_mean * scale
        return fused_bn_relu.fused_bn_relu_cuda(
            x.contiguous(),
            scale.contiguous(),
            shift.contiguous(),
        )

class ModelNew(nn.Module):
    def __init__(self, num_classes=1000, input_channels=3, alpha=1.0):
        super().__init__()

        def conv_bn(inp, oup, stride):
            return nn.Sequential(
                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),
                FusedBNReLU(oup),
            )

        def conv_dw(inp, oup, stride):
            return nn.Sequential(
                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),
                FusedBNReLU(inp),
                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
                FusedBNReLU(oup),
            )

        self.model = nn.Sequential(
            conv_bn(input_channels,             int(32   * alpha), 2),
            conv_dw(int(32   * alpha),          int(64   * alpha), 1),
            conv_dw(int(64   * alpha),          int(128  * alpha), 2),
            conv_dw(int(128  * alpha),          int(128  * alpha), 1),
            conv_dw(int(128  * alpha),          int(256  * alpha), 2),
            conv_dw(int(256  * alpha),          int(256  * alpha), 1),
            conv_dw(int(256  * alpha),          int(512  * alpha), 2),
            conv_dw(int(512  * alpha),          int(512  * alpha), 1),
            conv_dw(int(512  * alpha),          int(512  * alpha), 1),
            conv_dw(int(512  * alpha),          int(512  * alpha), 1),
            conv_dw(int(512  * alpha),          int(512  * alpha), 1),
            conv_dw(int(512  * alpha),          int(512  * alpha), 1),
            conv_dw(int(512  * alpha),          int(1024 * alpha), 2),
            conv_dw(int(1024 * alpha),          int(1024 * alpha), 1),
            nn.AvgPool2d(7),
        )
        self.fc = nn.Linear(int(1024 * alpha), num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.model(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)