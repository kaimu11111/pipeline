```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ----------------------------------------------------------------------
# Inline-compiled CUDA kernel: fast ReLU (element-wise, out-of-place)
# ----------------------------------------------------------------------
cpp_src = """
torch::Tensor relu_cuda(torch::Tensor input);
"""

cuda_src = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void relu_kernel(const float* __restrict__ input,
                            float* __restrict__ output,
                            const long size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size)
    {
        float v = input[idx];
        output[idx] = v > 0.f ? v : 0.f;
    }
}

torch::Tensor relu_cuda(torch::Tensor input)
{
    TORCH_CHECK(input.is_cuda(), "input must reside on GPU");
    TORCH_CHECK(input.scalar_type() == torch::kFloat32,
                "only float32 tensors are supported");

    auto output = torch::empty_like(input);
    const long size = input.numel();

    const int threads = 256;
    const int blocks  = (size + threads - 1) / threads;

    relu_kernel<<<blocks, threads>>>(input.data_ptr<float>(),
                                     output.data_ptr<float>(),
                                     size);
    return output;
}
"""

fast_relu = load_inline(
    name        = "fast_relu",
    cpp_sources = cpp_src,
    cuda_sources= cuda_src,
    functions   = ["relu_cuda"],
    verbose     = False,
)

# ----------------------------------------------------------------------
# Python helpers that wrap the CUDA kernel in torch.nn-style Modules
# ----------------------------------------------------------------------
class ReLUFast(nn.Module):
    def forward(self, x):
        return fast_relu.relu_cuda(x)

# ----------------------------------------------------------------------
# Fire module with custom ReLU
# ----------------------------------------------------------------------
class FireModuleNew(nn.Module):
    def __init__(self, in_channels, squeeze_channels,
                 expand1x1_channels, expand3x3_channels):
        super().__init__()
        self.squeeze   = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1)
        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1)
        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels,
                                   kernel_size=3, padding=1)
        self.relu = ReLUFast()

    def forward(self, x):
        x    = self.relu(self.squeeze(x))
        out1 = self.relu(self.expand1x1(x))
        out3 = self.relu(self.expand3x3(x))
        return torch.cat([out1, out3], 1)

# ----------------------------------------------------------------------
# Optimised model
# ----------------------------------------------------------------------
class ModelNew(nn.Module):
    def __init__(self, num_classes=1000):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=7, stride=2),
            ReLUFast(),
            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
            FireModuleNew(96, 16, 64, 64),
            FireModuleNew(128, 16, 64, 64),
            FireModuleNew(128, 32, 128, 128),
            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
            FireModuleNew(256, 32, 128, 128),
            FireModuleNew(256, 48, 192, 192),
            FireModuleNew(384, 48, 192, 192),
            FireModuleNew(384, 64, 256, 256),
            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),
            FireModuleNew(512, 64, 256, 256),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.0),
            nn.Conv2d(512, num_classes, kernel_size=1),
            ReLUFast(),
            nn.AdaptiveAvgPool2d((1, 1)),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return torch.flatten(x, 1)
```