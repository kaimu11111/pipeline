```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------------
# Inline CUDA kernel: compute per-sample mean over all dimensions except batch
# ---------------------------------------------------------------------------
cuda_src = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Kernel: one block per sample (batch element), thread-level strided reduction
__global__ void batch_mean_kernel(const float* __restrict__ x,
                                  float* __restrict__ out,
                                  const int inner_size)
{
    const int n = blockIdx.x;                          // sample index
    float thread_sum = 0.0f;

    // Strided reduction over this sample's elements
    for (int idx = threadIdx.x; idx < inner_size; idx += blockDim.x) {
        thread_sum += x[n * inner_size + idx];
    }

    // Block-level reduction (assumes blockDim.x <= 1024)
    extern __shared__ float shared[];
    shared[threadIdx.x] = thread_sum;
    __syncthreads();

    for (int stride = blockDim.x >> 1; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride)
            shared[threadIdx.x] += shared[threadIdx.x + stride];
        __syncthreads();
    }

    // Write result (normalised by number of elements)
    if (threadIdx.x == 0)
        out[n] = shared[0] / static_cast<float>(inner_size);
}

// C++/CUDA interface --------------------------------------------------------
torch::Tensor batch_mean_cuda(torch::Tensor x) {
    TORCH_CHECK(x.is_cuda(), "Input must reside on CUDA");
    TORCH_CHECK(x.scalar_type() == torch::kFloat32,
                "Only fp32 tensors are supported");

    const int64_t N = x.size(0);
    const int64_t inner_size = x.numel() / N;

    auto out = torch::empty({N}, x.options().dtype(torch::kFloat32));

    const int threads = 256;
    const dim3 blocks(N);
    const size_t shared_bytes = threads * sizeof(float);

    batch_mean_kernel<<<blocks, threads, shared_bytes>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        static_cast<int>(inner_size));

    return out;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("batch_mean_cuda", &batch_mean_cuda,
          "Per-sample mean over all dims except batch (CUDA)");
}
"""

cpp_decl = "torch::Tensor batch_mean_cuda(torch::Tensor x);"

# Build the extension
batch_mean_ext = load_inline(
    name="batch_mean_ext",
    cpp_sources=cpp_decl,
    cuda_sources=cuda_src,
    functions=["batch_mean_cuda"],
    verbose=False,
)

# ---------------------------------------------------------------------------
# Optimised model that uses the custom CUDA kernel
# ---------------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Model that performs a 3D convolution, applies Group Normalization,
    then computes the mean with a custom CUDA kernel.
    """
    def __init__(self, in_channels, out_channels, kernel_size, num_groups):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.group_norm = nn.GroupNorm(num_groups, out_channels)
        self.batch_mean = batch_mean_ext

    def forward(self, x):
        x = self.conv(x)
        x = self.group_norm(x)
        x = self.batch_mean.batch_mean_cuda(x)
        return x

# ---------------------------------------------------------------------------
# Helper functions to match original API
# ---------------------------------------------------------------------------
def get_inputs():
    batch_size = 64
    in_channels = 3
    D, H, W = 12, 16, 16
    return [torch.rand(batch_size, in_channels, D, H, W, device="cuda")]

def get_init_inputs():
    in_channels = 3
    out_channels = 24
    kernel_size = 3
    num_groups = 8
    return [in_channels, out_channels, kernel_size, num_groups]
```