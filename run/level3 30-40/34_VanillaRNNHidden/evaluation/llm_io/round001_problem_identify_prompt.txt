You are given:

ERROR_LOG:
/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Error building extension 'custom_rnn_kernels': [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=custom_rnn_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/custom_rnn_kernels/main.cpp -o main.o 
FAILED: main.o 
c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=custom_rnn_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/custom_rnn_kernels/main.cpp -o main.o 
/home/wan00559/.cache/torch_extensions/py311_cu126/custom_rnn_kernels/main.cpp: In function ‘void pybind11_init_custom_rnn_kernels(pybind11::module_&)’:
/home/wan00559/.cache/torch_extensions/py311_cu126/custom_rnn_kernels/main.cpp:4:62: error: ‘concat_linear_tanh_cuda’ was not declared in this scope
    4 | m.def("concat_linear_tanh_cuda", torch::wrap_pybind_function(concat_linear_tanh_cuda), "concat_linear_tanh_cuda");
      |                                                              ^~~~~~~~~~~~~~~~~~~~~~~
/home/wan00559/.cache/torch_extensions/py311_cu126/custom_rnn_kernels/main.cpp:5:50: error: ‘linear_cuda’ was not declared in this scope
    5 | m.def("linear_cuda", torch::wrap_pybind_function(linear_cuda), "linear_cuda");
      |                                                  ^~~~~~~~~~~
[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=custom_rnn_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/custom_rnn_kernels/cuda.cu -o cuda.cuda.o 
ninja: build stopped: subcommand failed.

PyTorch reference (ground truth):

import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """
        Initialize the Vanilla RNN model.

        :param input_size: The number of input features (int).
        :param hidden_size: The size of the hidden state (int).
        :param output_size: The number of output features (int).
        """
        super(Model, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Define the RNN cell components (input to hidden, hidden to hidden, and hidden to output)
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # Input to hidden
        self.h2o = nn.Linear(hidden_size, output_size)  # Hidden to output
        self.tanh = nn.Tanh()  # Activation function for hidden state

    def forward(self, x: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the Vanilla RNN.

        :param x: Input tensor of shape (seq_len, batch_size, input_size)
        :param h0: Initial hidden state tensor of shape (batch_size, hidden_size)
        :return: Output tensor of shape (seq_len, batch_size, output_size)
        """
        seq_len, batch_size, _ = x.size()
        hidden = h0.to(x.device)
        outputs = []

        for t in range(seq_len):
            combined = torch.cat((x[t], hidden), dim=1)  # Concatenate input and hidden state
            hidden = self.tanh(self.i2h(combined))  # Update hidden state
            output = self.h2o(hidden)  # Compute output
            outputs.append(output)

        return torch.stack(outputs, dim=0)  # (seq_len, batch_size, output_size)

# === Test configuration ===
batch_size = 4
input_size = 512
hidden_size = 128
output_size = 64
sequence_length = 128

def get_inputs():
    return [
        torch.rand(sequence_length, batch_size, input_size),
        torch.rand(batch_size, hidden_size)
    ]

def get_init_inputs():
    return [input_size, hidden_size, output_size]

CUDA candidate (to audit):

import math
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------
#              Hand–written CUDA kernels (inline compilation)
# ---------------------------------------------------------------------
cuda_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <vector>

///////////////////////////////////////////////////////////////////////////////
//  Kernel 1:  concat(input , hidden_prev) → linear → tanh  (update hidden)
///////////////////////////////////////////////////////////////////////////////
__global__ void concat_linear_tanh_kernel(
        const float* __restrict__ x,            // (B , I)
        const float* __restrict__ h_prev,       // (B , H)
        const float* __restrict__ weight,       // (H , I+H)   row–major
        const float* __restrict__ bias,         // (H)
        float*       __restrict__ h_next,       // (B , H)
        int  B, int I, int H)
{
    int b = blockIdx.x;      // batch dim
    int h = threadIdx.x;     // hidden dim
    if (b >= B || h >= H) return;

    int in_hidden = I + H;
    const float* w_row = weight + h * in_hidden;

    float acc = bias[h];

    // dot with input x
    for (int k = 0; k < I; ++k)
        acc += w_row[k] * x[b * I + k];

    // dot with previous hidden
    for (int k = 0; k < H; ++k)
        acc += w_row[I + k] * h_prev[b * H + k];

    h_next[b * H + h] = tanhf(acc);
}

torch::Tensor concat_linear_tanh_cuda(torch::Tensor x,
                                      torch::Tensor h_prev,
                                      torch::Tensor weight,
                                      torch::Tensor bias)
{
    TORCH_CHECK(x.is_cuda(), "x must be CUDA");
    TORCH_CHECK(h_prev.is_cuda(), "h_prev must be CUDA");
    TORCH_CHECK(weight.is_cuda(), "weight must be CUDA");
    TORCH_CHECK(bias.is_cuda(), "bias must be CUDA");

    const int B = x.size(0);
    const int I = x.size(1);
    const int H = h_prev.size(1);

    auto h_next = torch::empty_like(h_prev);

    dim3 grid(B);
    dim3 block(H);
    concat_linear_tanh_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        h_prev.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        h_next.data_ptr<float>(),
        B, I, H);

    return h_next;
}

///////////////////////////////////////////////////////////////////////////////
//  Kernel 2:  Standard linear layer  y = x W^T + b
///////////////////////////////////////////////////////////////////////////////
__global__ void linear_kernel(
        const float* __restrict__ x,         // (B , Fin)
        const float* __restrict__ weight,    // (Fout , Fin) row–major
        const float* __restrict__ bias,      // (Fout)
        float*       __restrict__ y,         // (B , Fout)
        int B, int Fin, int Fout)
{
    int b = blockIdx.x;
    int o = threadIdx.x;
    if (b >= B || o >= Fout) return;

    const float* w_row = weight + o * Fin;
    float acc = bias[o];
    for (int k = 0; k < Fin; ++k)
        acc += w_row[k] * x[b * Fin + k];

    y[b * Fout + o] = acc;
}

torch::Tensor linear_cuda(torch::Tensor x,
                          torch::Tensor weight,
                          torch::Tensor bias)
{
    TORCH_CHECK(x.is_cuda(), "x must be CUDA");
    TORCH_CHECK(weight.is_cuda(), "weight must be CUDA");
    TORCH_CHECK(bias.is_cuda(), "bias must be CUDA");

    const int B    = x.size(0);
    const int Fin  = x.size(1);
    const int Fout = weight.size(0);

    auto y = torch::empty({B, Fout}, x.options());

    dim3 grid(B);
    dim3 block(Fout);
    linear_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        y.data_ptr<float>(),
        B, Fin, Fout);

    return y;
}

///////////////////////////////////////////////////////////////////////////////
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("concat_linear_tanh_cuda", &concat_linear_tanh_cuda,
          "Fused concat+linear+tanh (CUDA)");
    m.def("linear_cuda", &linear_cuda,
          "Linear layer (CUDA)");
}
"""

cuda_functions = ["concat_linear_tanh_cuda", "linear_cuda"]

# Inline compile/load
rnn_cuda = load_inline(
    name="custom_rnn_kernels",
    cpp_sources="",
    cuda_sources=cuda_source,
    functions=cuda_functions,
    verbose=False,
)

# ---------------------------------------------------------------------
#                          Optimised Model
# ---------------------------------------------------------------------
class ModelNew(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """
        Optimised Vanilla RNN using custom CUDA kernels.
        """
        super().__init__()
        self.input_size  = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Parameters replacing i2h and h2o Linear layers
        self.W_i2h = nn.Parameter(torch.empty(hidden_size, input_size + hidden_size))
        self.b_i2h = nn.Parameter(torch.empty(hidden_size))

        self.W_h2o = nn.Parameter(torch.empty(output_size, hidden_size))
        self.b_h2o = nn.Parameter(torch.empty(output_size))

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.W_i2h, a=math.sqrt(5))
        nn.init.kaiming_uniform_(self.W_h2o, a=math.sqrt(5))

        fan_i2h = self.W_i2h.size(1)
        bound1  = 1 / math.sqrt(fan_i2h)
        nn.init.uniform_(self.b_i2h, -bound1, bound1)

        fan_h2o = self.W_h2o.size(1)
        bound2  = 1 / math.sqrt(fan_h2o)
        nn.init.uniform_(self.b_h2o, -bound2, bound2)

    def forward(self, x: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:
        """
        x  : (seq_len, batch, input_size)
        h0 : (batch, hidden_size)
        """
        assert x.is_cuda and h0.is_cuda, "Inputs must be on CUDA device"
        seq_len, batch_size, _ = x.size()

        hidden  = h0.contiguous()
        outputs = []

        for t in range(seq_len):
            # -- fused update of hidden state
            hidden = rnn_cuda.concat_linear_tanh_cuda(
                x[t].contiguous(),
                hidden,
                self.W_i2h,
                self.b_i2h
            )
            # -- output projection
            out = rnn_cuda.linear_cuda(
                hidden,
                self.W_h2o,
                self.b_h2o
            )
            outputs.append(out)

        return torch.stack(outputs, dim=0)

# ---------------------------------------------------------------------
# Interfaces required by the harness (unchanged)
# ---------------------------------------------------------------------
batch_size       = 4
input_size       = 512
hidden_size      = 128
output_size      = 64
sequence_length  = 128

def get_inputs():
    return [
        torch.rand(sequence_length, batch_size, input_size, device="cuda"),
        torch.rand(batch_size, hidden_size, device="cuda")
    ]

def get_init_inputs():
    return [input_size, hidden_size, output_size]


Follow the Rules and produce the JSON exactly in the specified format.