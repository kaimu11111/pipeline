```python
import math
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------
#              Hand–written CUDA kernels (inline compilation)
# ---------------------------------------------------------------------
cuda_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <vector>

///////////////////////////////////////////////////////////////////////////////
//  Kernel 1:  concat(input , hidden_prev) → linear → tanh  (update hidden)
///////////////////////////////////////////////////////////////////////////////
__global__ void concat_linear_tanh_kernel(
        const float* __restrict__ x,            // (B , I)
        const float* __restrict__ h_prev,       // (B , H)
        const float* __restrict__ weight,       // (H , I+H)   row–major
        const float* __restrict__ bias,         // (H)
        float*       __restrict__ h_next,       // (B , H)
        int  B, int I, int H)
{
    int b = blockIdx.x;      // batch dim
    int h = threadIdx.x;     // hidden dim
    if (b >= B || h >= H) return;

    int in_hidden = I + H;
    const float* w_row = weight + h * in_hidden;

    float acc = bias[h];

    // dot with input x
    for (int k = 0; k < I; ++k)
        acc += w_row[k] * x[b * I + k];

    // dot with previous hidden
    for (int k = 0; k < H; ++k)
        acc += w_row[I + k] * h_prev[b * H + k];

    h_next[b * H + h] = tanhf(acc);
}

torch::Tensor concat_linear_tanh_cuda(torch::Tensor x,
                                      torch::Tensor h_prev,
                                      torch::Tensor weight,
                                      torch::Tensor bias)
{
    TORCH_CHECK(x.is_cuda(), "x must be CUDA");
    TORCH_CHECK(h_prev.is_cuda(), "h_prev must be CUDA");
    TORCH_CHECK(weight.is_cuda(), "weight must be CUDA");
    TORCH_CHECK(bias.is_cuda(), "bias must be CUDA");

    const int B = x.size(0);
    const int I = x.size(1);
    const int H = h_prev.size(1);

    auto h_next = torch::empty_like(h_prev);

    dim3 grid(B);
    dim3 block(H);
    concat_linear_tanh_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        h_prev.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        h_next.data_ptr<float>(),
        B, I, H);

    return h_next;
}

///////////////////////////////////////////////////////////////////////////////
//  Kernel 2:  Standard linear layer  y = x W^T + b
///////////////////////////////////////////////////////////////////////////////
__global__ void linear_kernel(
        const float* __restrict__ x,         // (B , Fin)
        const float* __restrict__ weight,    // (Fout , Fin) row–major
        const float* __restrict__ bias,      // (Fout)
        float*       __restrict__ y,         // (B , Fout)
        int B, int Fin, int Fout)
{
    int b = blockIdx.x;
    int o = threadIdx.x;
    if (b >= B || o >= Fout) return;

    const float* w_row = weight + o * Fin;
    float acc = bias[o];
    for (int k = 0; k < Fin; ++k)
        acc += w_row[k] * x[b * Fin + k];

    y[b * Fout + o] = acc;
}

torch::Tensor linear_cuda(torch::Tensor x,
                          torch::Tensor weight,
                          torch::Tensor bias)
{
    TORCH_CHECK(x.is_cuda(), "x must be CUDA");
    TORCH_CHECK(weight.is_cuda(), "weight must be CUDA");
    TORCH_CHECK(bias.is_cuda(), "bias must be CUDA");

    const int B    = x.size(0);
    const int Fin  = x.size(1);
    const int Fout = weight.size(0);

    auto y = torch::empty({B, Fout}, x.options());

    dim3 grid(B);
    dim3 block(Fout);
    linear_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        y.data_ptr<float>(),
        B, Fin, Fout);

    return y;
}

///////////////////////////////////////////////////////////////////////////////
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("concat_linear_tanh_cuda", &concat_linear_tanh_cuda,
          "Fused concat+linear+tanh (CUDA)");
    m.def("linear_cuda", &linear_cuda,
          "Linear layer (CUDA)");
}
"""

cuda_functions = ["concat_linear_tanh_cuda", "linear_cuda"]

# Inline compile/load
rnn_cuda = load_inline(
    name="custom_rnn_kernels",
    cpp_sources="",
    cuda_sources=cuda_source,
    functions=cuda_functions,
    verbose=False,
)

# ---------------------------------------------------------------------
#                          Optimised Model
# ---------------------------------------------------------------------
class ModelNew(nn.Module):
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """
        Optimised Vanilla RNN using custom CUDA kernels.
        """
        super().__init__()
        self.input_size  = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Parameters replacing i2h and h2o Linear layers
        self.W_i2h = nn.Parameter(torch.empty(hidden_size, input_size + hidden_size))
        self.b_i2h = nn.Parameter(torch.empty(hidden_size))

        self.W_h2o = nn.Parameter(torch.empty(output_size, hidden_size))
        self.b_h2o = nn.Parameter(torch.empty(output_size))

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.W_i2h, a=math.sqrt(5))
        nn.init.kaiming_uniform_(self.W_h2o, a=math.sqrt(5))

        fan_i2h = self.W_i2h.size(1)
        bound1  = 1 / math.sqrt(fan_i2h)
        nn.init.uniform_(self.b_i2h, -bound1, bound1)

        fan_h2o = self.W_h2o.size(1)
        bound2  = 1 / math.sqrt(fan_h2o)
        nn.init.uniform_(self.b_h2o, -bound2, bound2)

    def forward(self, x: torch.Tensor, h0: torch.Tensor) -> torch.Tensor:
        """
        x  : (seq_len, batch, input_size)
        h0 : (batch, hidden_size)
        """
        assert x.is_cuda and h0.is_cuda, "Inputs must be on CUDA device"
        seq_len, batch_size, _ = x.size()

        hidden  = h0.contiguous()
        outputs = []

        for t in range(seq_len):
            # -- fused update of hidden state
            hidden = rnn_cuda.concat_linear_tanh_cuda(
                x[t].contiguous(),
                hidden,
                self.W_i2h,
                self.b_i2h
            )
            # -- output projection
            out = rnn_cuda.linear_cuda(
                hidden,
                self.W_h2o,
                self.b_h2o
            )
            outputs.append(out)

        return torch.stack(outputs, dim=0)

# ---------------------------------------------------------------------
# Interfaces required by the harness (unchanged)
# ---------------------------------------------------------------------
batch_size       = 4
input_size       = 512
hidden_size      = 128
output_size      = 64
sequence_length  = 128

def get_inputs():
    return [
        torch.rand(sequence_length, batch_size, input_size, device="cuda"),
        torch.rand(batch_size, hidden_size, device="cuda")
    ]

def get_init_inputs():
    return [input_size, hidden_size, output_size]
```