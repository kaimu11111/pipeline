You are a senior CUDA-extension developer.
Your job is to **FIX** the compilation or runtime errors in the Python script
shown below.

OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
2. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

────────────────────────────────────────
ERROR LOG
────────────────────────────────────────
/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Error building extension 'fused_gru_cell': [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=fused_gru_cell -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_gru_cell/main.cpp -o main.o 
[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=fused_gru_cell -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_gru_cell/cuda.cu -o cuda.cuda.o 
[3/3] c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_gru_cell.so
FAILED: fused_gru_cell.so 
c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_gru_cell.so
/usr/bin/ld: cuda.cuda.o: in function `PyInit_fused_gru_cell':
tmpxft_0001dde0_00000000-6_cuda.cudafe1.cpp:(.text+0x504): multiple definition of `PyInit_fused_gru_cell'; main.o:main.cpp:(.text+0x257): first defined here
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.

────────────────────────────────────────
OLD CODE (read-only)
────────────────────────────────────────
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ---------------------------------------------------------------------------
# CUDA kernel + host implementation
# ---------------------------------------------------------------------------
source = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__device__ __forceinline__ float sigmoidf(float x){
    return 1.f / (1.f + expf(-x));
}

__global__ void gru_cell_kernel(const float* __restrict__ gates_x,   // (B,3H)
                                const float* __restrict__ gates_h,   // (B,3H)
                                const float* __restrict__ h_prev,    // (B,H)
                                float* __restrict__ h_new,           // (B,H)
                                int hidden_size, int batch_size){
    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
    const int H   = hidden_size;
    const int N   = batch_size * H;
    if(idx >= N) return;

    const int b = idx / H;              // batch index
    const int h = idx - b * H;          // hidden index

    const int base   = b * 3 * H + h;   // offset for r gate
    const int base_z = base + H;        // offset for z gate
    const int base_n = base + 2*H;      // offset for n gate

    // retrieve gate pre-activations
    const float x_r = gates_x[base];
    const float x_z = gates_x[base_z];
    const float x_n = gates_x[base_n];

    const float h_r = gates_h[base];
    const float h_z = gates_h[base_z];
    const float h_n = gates_h[base_n];

    // element-wise GRU equations
    const float r   = sigmoidf(x_r + h_r);
    const float z   = sigmoidf(x_z + h_z);
    const float n   = tanhf(x_n + r * h_n);

    const float prev = h_prev[idx];
    const float out  = (1.f - z) * n + z * prev;

    h_new[idx] = out;
}

// Python–visible wrapper
torch::Tensor gru_cell_forward(torch::Tensor gates_x,
                               torch::Tensor gates_h,
                               torch::Tensor h_prev){
    TORCH_CHECK(gates_x.is_cuda(), "gates_x must be a CUDA tensor");
    TORCH_CHECK(gates_h.is_cuda(), "gates_h must be a CUDA tensor");
    TORCH_CHECK(h_prev.is_cuda(),  "h_prev must be a CUDA tensor");

    const int batch_size  = h_prev.size(0);
    const int hidden_size = h_prev.size(1);

    auto h_new = torch::empty_like(h_prev);

    const int total   = batch_size * hidden_size;
    const int threads = 256;
    const int blocks  = (total + threads - 1) / threads;

    gru_cell_kernel<<<blocks, threads>>>(
        gates_x.data_ptr<float>(),
        gates_h.data_ptr<float>(),
        h_prev.data_ptr<float>(),
        h_new.data_ptr<float>(),
        hidden_size,
        batch_size);

    return h_new;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m){
    m.def("gru_cell_forward",
          &gru_cell_forward,
          "Fused GRU-cell forward (CUDA)");
}
"""

# ---------------------------------------------------------------------------
# C++ prototypes required by the auto-generated binding file
# ---------------------------------------------------------------------------
cpp_src = r"""
#include <torch/extension.h>

torch::Tensor gru_cell_forward(torch::Tensor gates_x,
                               torch::Tensor gates_h,
                               torch::Tensor h_prev);
"""

# ---------------------------------------------------------------------------
# Build / load the extension
# ---------------------------------------------------------------------------
fused_gru_cell = load_inline(
    name="fused_gru_cell",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["gru_cell_forward"],
    verbose=False,
)

# ---------------------------------------------------------------------------
# PyTorch module using the fused kernel
# ---------------------------------------------------------------------------
class ModelNew(nn.Module):
    def __init__(self,
                 input_size,
                 hidden_size,
                 num_layers=3,
                 bias=True,
                 batch_first=False):
        super().__init__()
        self.input_size   = input_size
        self.hidden_size  = hidden_size
        self.num_layers   = num_layers
        self.bias         = bias
        self.batch_first  = batch_first

        self.weight_ih = nn.ParameterList()
        self.weight_hh = nn.ParameterList()
        if bias:
            self.bias_ih = nn.ParameterList()
            self.bias_hh = nn.ParameterList()
        else:
            self.register_parameter("bias_ih", None)
            self.register_parameter("bias_hh", None)

        for l in range(num_layers):
            in_feat = input_size if l == 0 else hidden_size
            self.weight_ih.append(nn.Parameter(torch.empty(3 * hidden_size, in_feat)))
            self.weight_hh.append(nn.Parameter(torch.empty(3 * hidden_size, hidden_size)))
            if bias:
                self.bias_ih.append(nn.Parameter(torch.empty(3 * hidden_size)))
                self.bias_hh.append(nn.Parameter(torch.empty(3 * hidden_size)))

        self.reset_parameters()

        # expose the CUDA kernel
        self.gru_cell_cuda = fused_gru_cell.gru_cell_forward

    def reset_parameters(self):
        for w in self.weight_ih:
            nn.init.xavier_uniform_(w)
        for w in self.weight_hh:
            nn.init.orthogonal_(w)
        if self.bias:
            for b in self.bias_ih:
                nn.init.zeros_(b)
            for b in self.bias_hh:
                nn.init.zeros_(b)

    def _gru_layer(self, x, h, layer_idx):
        """
        x : (seq_len, batch, input_size or hidden_size)
        h : (batch, hidden_size)
        """
        W_ih = self.weight_ih[layer_idx]
        W_hh = self.weight_hh[layer_idx]
        b_ih = self.bias_ih[layer_idx] if self.bias else None
        b_hh = self.bias_hh[layer_idx] if self.bias else None

        seq_len, _, _ = x.shape
        outputs = []

        for t in range(seq_len):
            x_t     = x[t]                                            # (B, F)
            gates_x = torch.nn.functional.linear(x_t, W_ih, b_ih)     # (B, 3H)
            gates_h = torch.nn.functional.linear(h,   W_hh, b_hh)     # (B, 3H)

            # launch fused CUDA kernel
            h = self.gru_cell_cuda(
                gates_x.contiguous(),
                gates_h.contiguous(),
                h.contiguous()
            )
            outputs.append(h.unsqueeze(0))

        return torch.cat(outputs, 0), h

    def forward(self, x, h_0):
        """
        x   : (seq_len, batch, input_size)  if batch_first = False
        h_0 : (num_layers, batch, hidden_size)
        """
        # ensure all inputs are on the same CUDA device as the parameters
        device = self.weight_ih[0].device
        x   = x.to(device)
        h_0 = h_0.to(device)

        if self.batch_first:
            x = x.transpose(0, 1)  # (seq_len, batch, feat)

        h_n_all   = []
        layer_inp = x

        for l in range(self.num_layers):
            h_l0             = h_0[l]                        # (B, H)
            layer_out, h_lN  = self._gru_layer(layer_inp, h_l0, l)
            h_n_all.append(h_lN.unsqueeze(0))
            layer_inp        = layer_out                     # feed to next layer

        output = layer_out                                   # (seq_len, batch, H)
        h_n    = torch.cat(h_n_all, 0)                       # (num_layers, batch, H)

        if self.batch_first:
            output = output.transpose(0, 1)                  # (batch, seq_len, H)

        return output, h_n

────────────────────────────────────────
Main Critical Problem
────────────────────────────────────────
critical_issue: CUDA file redeclares PYBIND11_MODULE, duplicating auto-generated one
why_it_matters: Two PyInit_fused_gru_cell symbols clash at link time, extension fails to build so model cannot run at all.
minimal_fix_hint: Delete PYBIND11_MODULE block in cuda.cu

```python
# <your corrected code>
```
# ==========================================================
