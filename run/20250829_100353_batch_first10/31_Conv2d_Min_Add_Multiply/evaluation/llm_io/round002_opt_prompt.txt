# ----------  Previously generated kernels ----------
## Existing kernels
### Kernel 1 · kernel_20250829_100432.py
```cuda
# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# --------------------------------------------------------------------
# CUDA kernel: fuse min-with-constant + channel-bias add + scalar scale
# --------------------------------------------------------------------
cuda_src = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__global__ void fuse_kernel(const float* __restrict__ x,
                            const float* __restrict__ bias,
                            float* __restrict__ out,
                            const int64_t total_elements,
                            const int64_t channels,
                            const int64_t spatial_size,
                            const float  constant_val,
                            const float  scale) {
    const int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= total_elements) return;

    // Recover channel index from flattened NCHW index
    const int64_t c = (idx / spatial_size) % channels;

    float v = x[idx];
    v = v < constant_val ? v : constant_val;   // min with constant
    v += bias[c];                              // add per-channel bias
    v *= scale;                                // multiply by scalar
    out[idx] = v;
}

torch::Tensor fuse_min_bias_scale_cuda(torch::Tensor x,
                                       torch::Tensor bias,
                                       float constant_val,
                                       float scale) {
    TORCH_CHECK(x.is_cuda(),  "Input tensor must be CUDA");
    TORCH_CHECK(bias.is_cuda(), "Bias tensor must be CUDA");
    TORCH_CHECK(x.dtype() == torch::kFloat32, "Only float32 supported");
    TORCH_CHECK(bias.dtype() == torch::kFloat32, "Only float32 bias supported");

    x = x.contiguous();
    bias = bias.contiguous().view({-1});   // flatten bias (C)

    auto out = torch::empty_like(x);

    const int64_t total_elements = x.numel();
    const int64_t channels       = x.size(1);
    const int64_t spatial_size   = x.size(2) * x.size(3); // H * W

    const int threads = 256;
    const int blocks  = (total_elements + threads - 1) / threads;

    fuse_kernel<<<blocks, threads>>>(x.data_ptr<float>(),
                                     bias.data_ptr<float>(),
                                     out.data_ptr<float>(),
                                     total_elements,
                                     channels,
                                     spatial_size,
                                     constant_val,
                                     scale);

    return out;
}
"""

cpp_decl = """
torch::Tensor fuse_min_bias_scale_cuda(torch::Tensor x,
                                       torch::Tensor bias,
                                       float constant_val,
                                       float scale);
"""

# Compile / load the inline extension (done once at import time)
_fused_ops = load_inline(name="fuse_min_bias_scale",
                         cpp_sources=cpp_decl,
                         cuda_sources=cuda_src,
                         functions=["fuse_min_bias_scale_cuda"],
                         verbose=False)

# --------------------
# Optimised torch model
# --------------------
class ModelNew(nn.Module):
    """
    Optimised model that keeps the convolution in PyTorch but fuses the
    subsequent min(+constant) + bias add + scalar multiplication into a
    single custom CUDA kernel.
    """
    def __init__(self, in_channels, out_channels,
                 kernel_size, constant_value,
                 bias_shape, scaling_factor):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.register_buffer("constant_value", torch.tensor(float(constant_value)))
        self.register_buffer("scaling_factor", torch.tensor(float(scaling_factor)))
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv(x)
        x = _fused_ops.fuse_min_bias_scale_cuda(
            x,
            self.bias.view(-1),
            float(self.constant_value),
            float(self.scaling_factor)
        )
        return x
```

### Kernel 2 · kernel_20250829_100625.py
```cuda
# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# --------------------------------------------------------------------
# CUDA kernel: optimised fuse min-with-constant + bias add + scalar scale
# --------------------------------------------------------------------
cuda_src = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Each block processes one (N, C) pair and iterates over the spatial
// dimension. This removes expensive div/mod operations that existed in
// the flat-index kernel and improves cache locality.
__global__ void fuse_kernel_v2(const float* __restrict__ x,
                               const float* __restrict__ bias,
                               float* __restrict__ out,
                               const int64_t spatial_size,
                               const int64_t channels,
                               const float   constant_val,
                               const float   scale) {
    const int c = blockIdx.x;   // channel
    const int n = blockIdx.y;   // batch index
    if (c >= channels) return;

    const float b = bias[c];
    const int64_t base = ( (int64_t)n * channels + c ) * spatial_size;

    // Iterate over spatial elements with a grid-stride loop
    for (int64_t s = threadIdx.x; s < spatial_size; s += blockDim.x) {
        const int64_t idx = base + s;
        float v = x[idx];
        v = v < constant_val ? v : constant_val;  // min with constant
        v += b;                                   // add bias
        v *= scale;                               // scale
        out[idx] = v;
    }
}

// Host wrapper
torch::Tensor fuse_min_bias_scale_cuda(torch::Tensor x,
                                       torch::Tensor bias,
                                       float constant_val,
                                       float scale) {
    TORCH_CHECK(x.is_cuda(),   "Input tensor must be CUDA");
    TORCH_CHECK(bias.is_cuda(),"Bias tensor must be CUDA");
    TORCH_CHECK(x.dtype() == torch::kFloat32,   "Only float32 supported");
    TORCH_CHECK(bias.dtype() == torch::kFloat32,"Only float32 bias supported");

    x    = x.contiguous();
    bias = bias.contiguous().view({-1});  // (C)

    auto out = torch::empty_like(x);

    const int64_t N         = x.size(0);
    const int64_t C         = x.size(1);
    const int64_t spatial   = x.size(2) * x.size(3);  // H*W

    const dim3 block(256);
    const dim3 grid(C, N);  // one block per (n, c)

    fuse_kernel_v2<<<grid, block>>>(x.data_ptr<float>(),
                                    bias.data_ptr<float>(),
                                    out.data_ptr<float>(),
                                    spatial,
                                    C,
                                    constant_val,
                                    scale);
    return out;
}
"""

cpp_src = """
torch::Tensor fuse_min_bias_scale_cuda(torch::Tensor x,
                                       torch::Tensor bias,
                                       float constant_val,
                                       float scale);
"""

# Compile / load the inline extension (done once at import time)
_fused_ops = load_inline(name="fuse_min_bias_scale_v2",
                         cpp_sources=cpp_src,
                         cuda_sources=cuda_src,
                         functions=["fuse_min_bias_scale_cuda"],
                         verbose=False)

# --------------------
# Optimised torch model
# --------------------
class ModelNew(nn.Module):
    """
    Optimised model that keeps the convolution in PyTorch but fuses the
    subsequent min(+constant) + bias add + scalar multiplication into a
    single custom CUDA kernel.
    """
    def __init__(self, in_channels, out_channels,
                 kernel_size, constant_value,
                 bias_shape, scaling_factor):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.register_buffer("constant_value", torch.tensor(float(constant_value)))
        self.register_buffer("scaling_factor", torch.tensor(float(scaling_factor)))
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv(x)
        x = _fused_ops.fuse_min_bias_scale_cuda(
            x,
            self.bias.view(-1),
            float(self.constant_value),
            float(self.scaling_factor)
        )
        return x
```


You are a CUDA-kernel optimization specialist.

Analyze the provided architecture and kernel history to produce an improved CUDA kernel.

[ARCHITECTURE FILE]
```python
# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# --------------------------------------------------------------------
# CUDA kernel: optimised fuse min-with-constant + bias add + scalar scale
# --------------------------------------------------------------------
cuda_src = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Each block processes one (N, C) pair and iterates over the spatial
// dimension. This removes expensive div/mod operations that existed in
// the flat-index kernel and improves cache locality.
__global__ void fuse_kernel_v2(const float* __restrict__ x,
                               const float* __restrict__ bias,
                               float* __restrict__ out,
                               const int64_t spatial_size,
                               const int64_t channels,
                               const float   constant_val,
                               const float   scale) {
    const int c = blockIdx.x;   // channel
    const int n = blockIdx.y;   // batch index
    if (c >= channels) return;

    const float b = bias[c];
    const int64_t base = ( (int64_t)n * channels + c ) * spatial_size;

    // Iterate over spatial elements with a grid-stride loop
    for (int64_t s = threadIdx.x; s < spatial_size; s += blockDim.x) {
        const int64_t idx = base + s;
        float v = x[idx];
        v = v < constant_val ? v : constant_val;  // min with constant
        v += b;                                   // add bias
        v *= scale;                               // scale
        out[idx] = v;
    }
}

// Host wrapper
torch::Tensor fuse_min_bias_scale_cuda(torch::Tensor x,
                                       torch::Tensor bias,
                                       float constant_val,
                                       float scale) {
    TORCH_CHECK(x.is_cuda(),   "Input tensor must be CUDA");
    TORCH_CHECK(bias.is_cuda(),"Bias tensor must be CUDA");
    TORCH_CHECK(x.dtype() == torch::kFloat32,   "Only float32 supported");
    TORCH_CHECK(bias.dtype() == torch::kFloat32,"Only float32 bias supported");

    x    = x.contiguous();
    bias = bias.contiguous().view({-1});  // (C)

    auto out = torch::empty_like(x);

    const int64_t N         = x.size(0);
    const int64_t C         = x.size(1);
    const int64_t spatial   = x.size(2) * x.size(3);  // H*W

    const dim3 block(256);
    const dim3 grid(C, N);  // one block per (n, c)

    fuse_kernel_v2<<<grid, block>>>(x.data_ptr<float>(),
                                    bias.data_ptr<float>(),
                                    out.data_ptr<float>(),
                                    spatial,
                                    C,
                                    constant_val,
                                    scale);
    return out;
}
"""

cpp_src = """
torch::Tensor fuse_min_bias_scale_cuda(torch::Tensor x,
                                       torch::Tensor bias,
                                       float constant_val,
                                       float scale);
"""

# Compile / load the inline extension (done once at import time)
_fused_ops = load_inline(name="fuse_min_bias_scale_v2",
                         cpp_sources=cpp_src,
                         cuda_sources=cuda_src,
                         functions=["fuse_min_bias_scale_cuda"],
                         verbose=False)

# --------------------
# Optimised torch model
# --------------------
class ModelNew(nn.Module):
    """
    Optimised model that keeps the convolution in PyTorch but fuses the
    subsequent min(+constant) + bias add + scalar multiplication into a
    single custom CUDA kernel.
    """
    def __init__(self, in_channels, out_channels,
                 kernel_size, constant_value,
                 bias_shape, scaling_factor):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.register_buffer("constant_value", torch.tensor(float(constant_value)))
        self.register_buffer("scaling_factor", torch.tensor(float(scaling_factor)))
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        x = self.conv(x)
        x = _fused_ops.fuse_min_bias_scale_cuda(
            x,
            self.bias.view(-1),
            float(self.constant_value),
            float(self.scaling_factor)
        )
        return x
```

GOAL
────
- Improve latency and throughput on the target GPU.
- Maintain correctness within atol=1e-4 or rtol=1e-4.
- Preserve the public Python API (same inputs/outputs, shapes, dtypes).


OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
2. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

```python
# <your corrected code>
```
# ==========================================================
