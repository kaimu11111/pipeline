You are a senior CUDA-extension developer.
Your job is to **FIX** the compilation or runtime errors in the Python script
shown below.

OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
2. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

────────────────────────────────────────
ERROR LOG
────────────────────────────────────────
/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Error building extension 'fused_pool_hardtanh_mean_tanh': [1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=fused_pool_hardtanh_mean_tanh -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_pool_hardtanh_mean_tanh/main.cpp -o main.o 
[2/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=fused_pool_hardtanh_mean_tanh -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include -isystem /home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/wan00559/miniconda3/envs/maxk/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/wan00559/.cache/torch_extensions/py311_cu126/fused_pool_hardtanh_mean_tanh/cuda.cu -o cuda.cuda.o 
[3/3] c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_pool_hardtanh_mean_tanh.so
FAILED: fused_pool_hardtanh_mean_tanh.so 
c++ main.o cuda.cuda.o -shared -L/home/wan00559/miniconda3/envs/maxk/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_pool_hardtanh_mean_tanh.so
/usr/bin/ld: cuda.cuda.o: in function `PyInit_fused_pool_hardtanh_mean_tanh':
tmpxft_0009820b_00000000-6_cuda.cudafe1.cpp:(.text+0x61b): multiple definition of `PyInit_fused_pool_hardtanh_mean_tanh'; main.o:main.cpp:(.text+0x257): first defined here
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.

────────────────────────────────────────
OLD CODE (read-only)
────────────────────────────────────────
# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ----------------------------------------------------------------------
# Fused CUDA kernel: MaxPool2d (k=2,s=2) + HardTanh + Mean + Tanh
# ----------------------------------------------------------------------
cuda_src = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

////////////////////////////////////////////////////////////////////////////////
// CUDA kernel
////////////////////////////////////////////////////////////////////////////////
__global__ void fused_pool_hardtanh_mean_tanh_kernel(
        const float* __restrict__ x,   // [N, C, H, W]
        float* __restrict__ out,       // [N, C, 1, 1]
        const int N, const int C,
        const int H, const int W,
        const float hard_min,
        const float hard_max) {

    // One (block) handles one (n,c) pair
    const int nc = blockIdx.x;
    if (nc >= N * C) return;

    const int n = nc / C;
    const int c = nc % C;

    const int pooled_H = H >> 1;   // H/2
    const int pooled_W = W >> 1;   // W/2
    const int pooled_size = pooled_H * pooled_W;

    // Strides to locate elements in NHWC layout (PyTorch is NCHW)
    const int stride_n = C * H * W;
    const int stride_c = H * W;
    const int stride_h = W;
    const int stride_w = 1;

    // Base pointer for (n,c,0,0)
    const int base_idx = n * stride_n + c * stride_c;

    // Each thread accumulates partial sum                               
    float partial_sum = 0.0f;
    for (int idx = threadIdx.x; idx < pooled_size; idx += blockDim.x) {
        const int ph = idx / pooled_W;          // pooled height index
        const int pw = idx % pooled_W;          // pooled width index

        const int h0 = (ph << 1);               // 2*ph
        const int h1 = h0 + 1;
        const int w0 = (pw << 1);               // 2*pw
        const int w1 = w0 + 1;

        // Fetch 4 values for 2x2 window
        const int idx00 = base_idx + h0 * stride_h + w0 * stride_w;
        const int idx01 = base_idx + h0 * stride_h + w1 * stride_w;
        const int idx10 = base_idx + h1 * stride_h + w0 * stride_w;
        const int idx11 = base_idx + h1 * stride_h + w1 * stride_w;

        float v0 = x[idx00];
        float v1 = x[idx01];
        float v2 = x[idx10];
        float v3 = x[idx11];

        // MaxPool2d (2x2)
        float max_val = fmaxf(fmaxf(v0, v1), fmaxf(v2, v3));

        // HardTanh
        float clamped = fminf(fmaxf(max_val, hard_min), hard_max);

        partial_sum += clamped;
    }

    // ------------------------------------------------------------------
    // Parallel reduction within the block
    // ------------------------------------------------------------------
    extern __shared__ float shm[];
    shm[threadIdx.x] = partial_sum;
    __syncthreads();

    for (int stride = blockDim.x >> 1; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shm[threadIdx.x] += shm[threadIdx.x + stride];
        }
        __syncthreads();
    }

    // Thread 0 writes the final result: mean + tanh
    if (threadIdx.x == 0) {
        float mean_val = shm[0] / static_cast<float>(pooled_size);
        out[nc] = tanhf(mean_val);
    }
}

////////////////////////////////////////////////////////////////////////////////
// C++/PyTorch binding
////////////////////////////////////////////////////////////////////////////////
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(
        torch::Tensor input,
        float hard_min,
        float hard_max) {

    TORCH_CHECK(input.is_cuda(), "Input tensor must be on CUDA device");
    TORCH_CHECK(input.scalar_type() == at::kFloat,
                "Only float32 tensors are supported");
    TORCH_CHECK(input.dim() == 4, "Input must be a 4-D tensor [N,C,H,W]");
    TORCH_CHECK((input.size(2) & 1) == 0 && (input.size(3) & 1) == 0,
                "H and W must be even for 2x2 pooling");

    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto out = torch::empty({N, C, 1, 1}, input.options());

    const int threads = 256;
    const int blocks  = N * C;
    const int sh_mem  = threads * sizeof(float);

    fused_pool_hardtanh_mean_tanh_kernel<<<blocks, threads, sh_mem>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W,
        hard_min,
        hard_max);

    return out;
}

////////////////////////////////////////////////////////////////////////////////
// Binding code
////////////////////////////////////////////////////////////////////////////////
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("fused_pool_hardtanh_mean_tanh_cuda",
          &fused_pool_hardtanh_mean_tanh_cuda,
          "Fused (MaxPool2d+HardTanh+Mean+Tanh) CUDA kernel");
}
"""

cpp_declaration = """
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(torch::Tensor input, float hard_min, float hard_max);
"""

# Compile & load the extension
fused_ops = load_inline(
    name="fused_pool_hardtanh_mean_tanh",
    cpp_sources=cpp_declaration,
    cuda_sources=cuda_src,
    functions=["fused_pool_hardtanh_mean_tanh_cuda"],
    verbose=False,
)

# ----------------------------------------------------------------------
# Optimised PyTorch module using the fused CUDA kernel
# ----------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimised model that keeps the ConvTranspose2d layer from the original
    architecture but fuses the subsequent MaxPool2d, HardTanh, Mean and Tanh
    operations into a single custom CUDA kernel.
    """
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size,
                 stride,
                 padding,
                 maxpool_kernel_size,
                 maxpool_stride,
                 hardtanh_min,
                 hardtanh_max):
        super().__init__()

        # We only keep ConvTranspose2d as-is; the rest is fused.
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
        )

        # Store HardTanh limits for use inside the fused kernel.
        self.hardtanh_min = float(hardtanh_min)
        self.hardtanh_max = float(hardtanh_max)

    def forward(self, x):
        # Step-1: ConvTranspose2d (native PyTorch)
        x = self.conv_transpose(x)

        # Step-2-5: Fused MaxPool2d + HardTanh + Mean + Tanh (custom CUDA)
        x = fused_ops.fused_pool_hardtanh_mean_tanh_cuda(
            x,
            self.hardtanh_min,
            self.hardtanh_max,
        )

        return x

# ----------------------------------------------------------------------
# Helper functions for external scripts
# ----------------------------------------------------------------------
batch_size = 64
in_channels  = 32
out_channels = 32
height = width = 128
kernel_size  = 3
stride = 1
padding = 1
maxpool_kernel_size = 2
maxpool_stride = 2
hardtanh_min = -1
hardtanh_max = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width, device='cuda')]

def get_init_inputs():
    return [
        in_channels, out_channels, kernel_size, stride, padding,
        maxpool_kernel_size, maxpool_stride,
        hardtanh_min, hardtanh_max
    ]

────────────────────────────────────────
Main Critical Problem
────────────────────────────────────────
critical_issue: Duplicate PYBIND11_MODULE definition in cuda_src and auto-generated stub
why_it_matters: Linker finds two PyInit_fused_pool_hardtanh_mean_tanh symbols, build fails, kernel never runs so model outputs nothing.
minimal_fix_hint: Delete PYBIND11_MODULE block from cuda_src

```python
# <your corrected code>
```
# ==========================================================
