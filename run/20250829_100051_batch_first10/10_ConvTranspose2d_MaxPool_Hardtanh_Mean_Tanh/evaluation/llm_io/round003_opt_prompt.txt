# ----------  Previously generated kernels ----------
## Existing kernels
### Kernel 1 · kernel_20250829_100132.py
```cuda
# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ----------------------------------------------------------------------
# Fused CUDA kernel: MaxPool2d (k=2,s=2) + HardTanh + Mean + Tanh
# ----------------------------------------------------------------------
cuda_src = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

////////////////////////////////////////////////////////////////////////////////
// CUDA kernel
////////////////////////////////////////////////////////////////////////////////
__global__ void fused_pool_hardtanh_mean_tanh_kernel(
        const float* __restrict__ x,   // [N, C, H, W]
        float* __restrict__ out,       // [N, C, 1, 1]
        const int N, const int C,
        const int H, const int W,
        const float hard_min,
        const float hard_max) {

    // One (block) handles one (n,c) pair
    const int nc = blockIdx.x;
    if (nc >= N * C) return;

    const int n = nc / C;
    const int c = nc % C;

    const int pooled_H = H >> 1;   // H/2
    const int pooled_W = W >> 1;   // W/2
    const int pooled_size = pooled_H * pooled_W;

    // Strides to locate elements in NHWC layout (PyTorch is NCHW)
    const int stride_n = C * H * W;
    const int stride_c = H * W;
    const int stride_h = W;
    const int stride_w = 1;

    // Base pointer for (n,c,0,0)
    const int base_idx = n * stride_n + c * stride_c;

    // Each thread accumulates partial sum                               
    float partial_sum = 0.0f;
    for (int idx = threadIdx.x; idx < pooled_size; idx += blockDim.x) {
        const int ph = idx / pooled_W;          // pooled height index
        const int pw = idx % pooled_W;          // pooled width index

        const int h0 = (ph << 1);               // 2*ph
        const int h1 = h0 + 1;
        const int w0 = (pw << 1);               // 2*pw
        const int w1 = w0 + 1;

        // Fetch 4 values for 2x2 window
        const int idx00 = base_idx + h0 * stride_h + w0 * stride_w;
        const int idx01 = base_idx + h0 * stride_h + w1 * stride_w;
        const int idx10 = base_idx + h1 * stride_h + w0 * stride_w;
        const int idx11 = base_idx + h1 * stride_h + w1 * stride_w;

        float v0 = x[idx00];
        float v1 = x[idx01];
        float v2 = x[idx10];
        float v3 = x[idx11];

        // MaxPool2d (2x2)
        float max_val = fmaxf(fmaxf(v0, v1), fmaxf(v2, v3));

        // HardTanh
        float clamped = fminf(fmaxf(max_val, hard_min), hard_max);

        partial_sum += clamped;
    }

    // ------------------------------------------------------------------
    // Parallel reduction within the block
    // ------------------------------------------------------------------
    extern __shared__ float shm[];
    shm[threadIdx.x] = partial_sum;
    __syncthreads();

    for (int stride = blockDim.x >> 1; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shm[threadIdx.x] += shm[threadIdx.x + stride];
        }
        __syncthreads();
    }

    // Thread 0 writes the final result: mean + tanh
    if (threadIdx.x == 0) {
        float mean_val = shm[0] / static_cast<float>(pooled_size);
        out[nc] = tanhf(mean_val);
    }
}

////////////////////////////////////////////////////////////////////////////////
// C++/PyTorch binding
////////////////////////////////////////////////////////////////////////////////
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(
        torch::Tensor input,
        float hard_min,
        float hard_max) {

    TORCH_CHECK(input.is_cuda(), "Input tensor must be on CUDA device");
    TORCH_CHECK(input.scalar_type() == at::kFloat,
                "Only float32 tensors are supported");
    TORCH_CHECK(input.dim() == 4, "Input must be a 4-D tensor [N,C,H,W]");
    TORCH_CHECK((input.size(2) & 1) == 0 && (input.size(3) & 1) == 0,
                "H and W must be even for 2x2 pooling");

    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto out = torch::empty({N, C, 1, 1}, input.options());

    const int threads = 256;
    const int blocks  = N * C;
    const int sh_mem  = threads * sizeof(float);

    fused_pool_hardtanh_mean_tanh_kernel<<<blocks, threads, sh_mem>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W,
        hard_min,
        hard_max);

    return out;
}

////////////////////////////////////////////////////////////////////////////////
// Binding code
////////////////////////////////////////////////////////////////////////////////
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("fused_pool_hardtanh_mean_tanh_cuda",
          &fused_pool_hardtanh_mean_tanh_cuda,
          "Fused (MaxPool2d+HardTanh+Mean+Tanh) CUDA kernel");
}
"""

cpp_declaration = """
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(torch::Tensor input, float hard_min, float hard_max);
"""

# Compile & load the extension
fused_ops = load_inline(
    name="fused_pool_hardtanh_mean_tanh",
    cpp_sources=cpp_declaration,
    cuda_sources=cuda_src,
    functions=["fused_pool_hardtanh_mean_tanh_cuda"],
    verbose=False,
)

# ----------------------------------------------------------------------
# Optimised PyTorch module using the fused CUDA kernel
# ----------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimised model that keeps the ConvTranspose2d layer from the original
    architecture but fuses the subsequent MaxPool2d, HardTanh, Mean and Tanh
    operations into a single custom CUDA kernel.
    """
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size,
                 stride,
                 padding,
                 maxpool_kernel_size,
                 maxpool_stride,
                 hardtanh_min,
                 hardtanh_max):
        super().__init__()

        # We only keep ConvTranspose2d as-is; the rest is fused.
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
        )

        # Store HardTanh limits for use inside the fused kernel.
        self.hardtanh_min = float(hardtanh_min)
        self.hardtanh_max = float(hardtanh_max)

    def forward(self, x):
        # Step-1: ConvTranspose2d (native PyTorch)
        x = self.conv_transpose(x)

        # Step-2-5: Fused MaxPool2d + HardTanh + Mean + Tanh (custom CUDA)
        x = fused_ops.fused_pool_hardtanh_mean_tanh_cuda(
            x,
            self.hardtanh_min,
            self.hardtanh_max,
        )

        return x

# ----------------------------------------------------------------------
# Helper functions for external scripts
# ----------------------------------------------------------------------
batch_size = 64
in_channels  = 32
out_channels = 32
height = width = 128
kernel_size  = 3
stride = 1
padding = 1
maxpool_kernel_size = 2
maxpool_stride = 2
hardtanh_min = -1
hardtanh_max = 1

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width, device='cuda')]

def get_init_inputs():
    return [
        in_channels, out_channels, kernel_size, stride, padding,
        maxpool_kernel_size, maxpool_stride,
        hardtanh_min, hardtanh_max
    ]
```

### Kernel 2 · kernel_20250829_100521.py
```cuda
# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ----------------------------------------------------------------------
# CUDA kernel + host wrapper (WITHOUT the PYBIND11_MODULE block)
# ----------------------------------------------------------------------
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

////////////////////////////////////////////////////////////////////////////////
// CUDA kernel
////////////////////////////////////////////////////////////////////////////////
__global__ void fused_pool_hardtanh_mean_tanh_kernel(
        const float* __restrict__ x,   // [N, C, H, W]
        float* __restrict__ out,       // [N, C, 1, 1]
        const int N, const int C,
        const int H, const int W,
        const float hard_min,
        const float hard_max) {

    // One (block) handles one (n,c) pair
    const int nc = blockIdx.x;
    if (nc >= N * C) return;

    const int n = nc / C;
    const int c = nc % C;

    const int pooled_H = H >> 1;   // H/2
    const int pooled_W = W >> 1;   // W/2
    const int pooled_size = pooled_H * pooled_W;

    // Strides for NCHW
    const int stride_n = C * H * W;
    const int stride_c = H * W;
    const int stride_h = W;

    // Base pointer for (n,c,0,0)
    const int base_idx = n * stride_n + c * stride_c;

    // Each thread accumulates partial sum
    float partial_sum = 0.0f;
    for (int idx = threadIdx.x; idx < pooled_size; idx += blockDim.x) {
        const int ph = idx / pooled_W;          // pooled height index
        const int pw = idx % pooled_W;          // pooled width index

        const int h0 = (ph << 1);               // 2*ph
        const int h1 = h0 + 1;
        const int w0 = (pw << 1);               // 2*pw
        const int w1 = w0 + 1;

        // Fetch 4 values for 2x2 window
        const int idx00 = base_idx + h0 * stride_h + w0;
        const int idx01 = base_idx + h0 * stride_h + w1;
        const int idx10 = base_idx + h1 * stride_h + w0;
        const int idx11 = base_idx + h1 * stride_h + w1;

        float v0 = x[idx00];
        float v1 = x[idx01];
        float v2 = x[idx10];
        float v3 = x[idx11];

        // MaxPool2d (2x2)
        float max_val = fmaxf(fmaxf(v0, v1), fmaxf(v2, v3));

        // HardTanh
        float clamped = fminf(fmaxf(max_val, hard_min), hard_max);

        partial_sum += clamped;
    }

    // ------------------------------------------------------------------
    // Parallel reduction within the block
    // ------------------------------------------------------------------
    extern __shared__ float shm[];
    shm[threadIdx.x] = partial_sum;
    __syncthreads();

    for (int stride = blockDim.x >> 1; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shm[threadIdx.x] += shm[threadIdx.x + stride];
        }
        __syncthreads();
    }

    // Thread 0 writes the final result: mean + tanh
    if (threadIdx.x == 0) {
        float mean_val = shm[0] / static_cast<float>(pooled_size);
        out[nc] = tanhf(mean_val);
    }
}

////////////////////////////////////////////////////////////////////////////////
// C++ / PyTorch binding (no PYBIND11_MODULE here – added automatically by
// torch.utils.cpp_extension::load_inline)
////////////////////////////////////////////////////////////////////////////////
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(
        torch::Tensor input,
        float hard_min,
        float hard_max) {

    TORCH_CHECK(input.is_cuda(), "Input tensor must be on CUDA device");
    TORCH_CHECK(input.scalar_type() == at::kFloat,
                "Only float32 tensors are supported");
    TORCH_CHECK(input.dim() == 4, "Input must be a 4-D tensor [N,C,H,W]");
    TORCH_CHECK((input.size(2) & 1) == 0 && (input.size(3) & 1) == 0,
                "H and W must be even for 2×2 pooling");

    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto out = torch::empty({N, C, 1, 1}, input.options());

    const int threads = 256;
    const int blocks  = N * C;
    const int sh_mem  = threads * sizeof(float);

    fused_pool_hardtanh_mean_tanh_kernel<<<blocks, threads, sh_mem>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W,
        hard_min,
        hard_max);

    return out;
}
"""

# ----------------------------------------------------------------------
# C++ prototypes exposed to Python
# ----------------------------------------------------------------------
cpp_src = """
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(torch::Tensor input,
                                                 float hard_min,
                                                 float hard_max);
"""

# ----------------------------------------------------------------------
# Build / load the extension
# ----------------------------------------------------------------------
fused_ops = load_inline(
    name="fused_pool_hardtanh_mean_tanh",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_pool_hardtanh_mean_tanh_cuda"],
    verbose=False,
)

# ----------------------------------------------------------------------
# PyTorch module that uses the fused CUDA kernel
# ----------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimised model that keeps the ConvTranspose2d layer from the original
    architecture but fuses MaxPool2d → HardTanh → Mean → Tanh into a single
    custom CUDA kernel.
    """
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size,
                 stride,
                 padding,
                 maxpool_kernel_size,
                 maxpool_stride,
                 hardtanh_min,
                 hardtanh_max):
        super().__init__()

        # Original ConvTranspose2d layer
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
        )

        # Store HardTanh limits for the fused kernel
        self.hardtanh_min = float(hardtanh_min)
        self.hardtanh_max = float(hardtanh_max)

    def forward(self, x):
        # 1. ConvTranspose2d (native)
        x = self.conv_transpose(x)

        # 2-5. Fused MaxPool2d + HardTanh + Mean + Tanh (CUDA)
        x = fused_ops.fused_pool_hardtanh_mean_tanh_cuda(
            x,
            self.hardtanh_min,
            self.hardtanh_max,
        )
        return x
```

### Kernel 3 · kernel_20250829_100726.py
```cuda
# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ----------------------------------------------------------------------
# CUDA kernel + host wrapper
# ----------------------------------------------------------------------
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

// ---------------------------------------------------------------------
// Utility: fast intra-warp reduction (sum)
// ---------------------------------------------------------------------
__inline__ __device__
float warp_reduce_sum(float val) {
    #pragma unroll
    for (int offset = 16; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

////////////////////////////////////////////////////////////////////////////////
// Fused kernel: (MaxPool2d k=2,s=2) → HardTanh → Mean → Tanh
// One CUDA block  processes exactly one (n,c) channel map.
// We rely on warp-shuffle intrinsics to minimise shared-mem traffic.
// ---------------------------------------------------------------------------
////////////////////////////////////////////////////////////////////////////////
__global__ void fused_pool_hardtanh_mean_tanh_kernel(
        const float* __restrict__ x,   // [N, C, H, W]
        float* __restrict__ out,       // [N, C, 1, 1]
        const int N, const int C,
        const int H, const int W,
        const float hard_min,
        const float hard_max) {

    const int nc = blockIdx.x;                    // flat(n,c)
    if (nc >= N * C) return;

    const int n = nc / C;
    const int c = nc % C;

    const int pooled_H  = H >> 1;                 // H/2
    const int pooled_W  = W >> 1;                 // W/2
    const int pooled_sz = pooled_H * pooled_W;    // elements AFTER pooling

    // NCHW strides
    const int stride_n = C * H * W;
    const int stride_c = H * W;

    const int base_idx = n * stride_n + c * stride_c;

    // ------------------------------------------------------------------
    // Accumulate clamped-max over all 2×2 windows assigned to this (n,c)
    // ------------------------------------------------------------------
    float local_sum = 0.f;

    for (int idx = threadIdx.x; idx < pooled_sz; idx += blockDim.x) {
        const int ph = idx / pooled_W;            // pooled height
        const int pw = idx - ph * pooled_W;       // pooled width (faster than %)

        // Top-left corner of 2×2 window in original feature map
        const int h0 = ph << 1;
        const int w0 = pw << 1;
        const int h1 = h0 + 1;
        const int w1 = w0 + 1;

        const int row0 = base_idx + h0 * W;
        const int row1 = base_idx + h1 * W;

        // Read 4 values – memory-coalesced along W dimension
        float v00 = x[row0 + w0];
        float v01 = x[row0 + w1];
        float v10 = x[row1 + w0];
        float v11 = x[row1 + w1];

        // MaxPool 2×2
        float max_val = fmaxf(fmaxf(v00, v01), fmaxf(v10, v11));

        // HardTanh
        float clamped = fminf(fmaxf(max_val, hard_min), hard_max);

        local_sum += clamped;
    }

    // ------------------------------------------------------------------
    // Block-wide reduction – first within warps via shuffle, then across
    // warps through a tiny shared buffer (≤8 floats for 256-thread block).
    // ------------------------------------------------------------------
    const int lane    = threadIdx.x & 31;         // thread’s lane id
    const int warp_id = threadIdx.x >> 5;         // warp id inside block

    float warp_sum = warp_reduce_sum(local_sum);

    // Shared memory to collect one partial per warp
    extern __shared__ float sdata[];
    if (lane == 0) sdata[warp_id] = warp_sum;
    __syncthreads();

    // Final reduction by first warp
    float block_sum = 0.f;
    if (warp_id == 0) {
        block_sum = (lane < (blockDim.x >> 5)) ? sdata[lane] : 0.f;
        block_sum = warp_reduce_sum(block_sum);

        if (lane == 0) {
            float mean_val = block_sum / static_cast<float>(pooled_sz);
            out[nc] = tanhf(mean_val);
        }
    }
}

////////////////////////////////////////////////////////////////////////////////
// Host wrapper exposed to Python
////////////////////////////////////////////////////////////////////////////////
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(
        torch::Tensor input,
        float hard_min,
        float hard_max) {

    TORCH_CHECK(input.is_cuda(), "Input tensor must be on CUDA device");
    TORCH_CHECK(input.scalar_type() == at::kFloat,
                "Only float32 tensors are supported");
    TORCH_CHECK(input.dim() == 4, "Input must be a 4-D tensor [N,C,H,W]");
    TORCH_CHECK((input.size(2) & 1) == 0 && (input.size(3) & 1) == 0,
                "H and W must be even for 2×2 pooling");

    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto out = torch::empty({N, C, 1, 1}, input.options());

    // Tunable launch parameters
    constexpr int threads = 256;                  // 8 warps / block
    const int blocks      = N * C;
    const int sh_mem      = (threads >> 5) * sizeof(float); // one float / warp

    fused_pool_hardtanh_mean_tanh_kernel<<<blocks, threads, sh_mem>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W,
        hard_min,
        hard_max);

    return out;
}
"""

# ----------------------------------------------------------------------
# C++ prototypes exposed to Python
# ----------------------------------------------------------------------
cpp_src = """
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(torch::Tensor input,
                                                 float hard_min,
                                                 float hard_max);
"""

# ----------------------------------------------------------------------
# Build / load the extension
# ----------------------------------------------------------------------
fused_ops = load_inline(
    name="fused_pool_hardtanh_mean_tanh",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_pool_hardtanh_mean_tanh_cuda"],
    verbose=False,
)

# ----------------------------------------------------------------------
# PyTorch module using the improved fused kernel
# ----------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Model with ConvTranspose2d followed by a highly-optimised fused
    (MaxPool2d → HardTanh → Mean → Tanh) CUDA kernel.
    The Python interface mirrors the original architecture.
    """
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size,
                 stride,
                 padding,
                 maxpool_kernel_size,
                 maxpool_stride,
                 hardtanh_min,
                 hardtanh_max):
        super().__init__()

        self.conv_transpose = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
        )

        self.hardtanh_min = float(hardtanh_min)
        self.hardtanh_max = float(hardtanh_max)

    def forward(self, x):
        # 1. Standard ConvTranspose2d
        x = self.conv_transpose(x)

        # 2-5. Fused CUDA kernel
        x = fused_ops.fused_pool_hardtanh_mean_tanh_cuda(
            x,
            self.hardtanh_min,
            self.hardtanh_max,
        )
        return x
```


You are a CUDA-kernel optimization specialist.

Analyze the provided architecture and kernel history to produce an improved CUDA kernel.

[ARCHITECTURE FILE]
```python
# <complete ModelNew code>
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# ----------------------------------------------------------------------
# CUDA kernel + host wrapper
# ----------------------------------------------------------------------
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

// ---------------------------------------------------------------------
// Utility: fast intra-warp reduction (sum)
// ---------------------------------------------------------------------
__inline__ __device__
float warp_reduce_sum(float val) {
    #pragma unroll
    for (int offset = 16; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

////////////////////////////////////////////////////////////////////////////////
// Fused kernel: (MaxPool2d k=2,s=2) → HardTanh → Mean → Tanh
// One CUDA block  processes exactly one (n,c) channel map.
// We rely on warp-shuffle intrinsics to minimise shared-mem traffic.
// ---------------------------------------------------------------------------
////////////////////////////////////////////////////////////////////////////////
__global__ void fused_pool_hardtanh_mean_tanh_kernel(
        const float* __restrict__ x,   // [N, C, H, W]
        float* __restrict__ out,       // [N, C, 1, 1]
        const int N, const int C,
        const int H, const int W,
        const float hard_min,
        const float hard_max) {

    const int nc = blockIdx.x;                    // flat(n,c)
    if (nc >= N * C) return;

    const int n = nc / C;
    const int c = nc % C;

    const int pooled_H  = H >> 1;                 // H/2
    const int pooled_W  = W >> 1;                 // W/2
    const int pooled_sz = pooled_H * pooled_W;    // elements AFTER pooling

    // NCHW strides
    const int stride_n = C * H * W;
    const int stride_c = H * W;

    const int base_idx = n * stride_n + c * stride_c;

    // ------------------------------------------------------------------
    // Accumulate clamped-max over all 2×2 windows assigned to this (n,c)
    // ------------------------------------------------------------------
    float local_sum = 0.f;

    for (int idx = threadIdx.x; idx < pooled_sz; idx += blockDim.x) {
        const int ph = idx / pooled_W;            // pooled height
        const int pw = idx - ph * pooled_W;       // pooled width (faster than %)

        // Top-left corner of 2×2 window in original feature map
        const int h0 = ph << 1;
        const int w0 = pw << 1;
        const int h1 = h0 + 1;
        const int w1 = w0 + 1;

        const int row0 = base_idx + h0 * W;
        const int row1 = base_idx + h1 * W;

        // Read 4 values – memory-coalesced along W dimension
        float v00 = x[row0 + w0];
        float v01 = x[row0 + w1];
        float v10 = x[row1 + w0];
        float v11 = x[row1 + w1];

        // MaxPool 2×2
        float max_val = fmaxf(fmaxf(v00, v01), fmaxf(v10, v11));

        // HardTanh
        float clamped = fminf(fmaxf(max_val, hard_min), hard_max);

        local_sum += clamped;
    }

    // ------------------------------------------------------------------
    // Block-wide reduction – first within warps via shuffle, then across
    // warps through a tiny shared buffer (≤8 floats for 256-thread block).
    // ------------------------------------------------------------------
    const int lane    = threadIdx.x & 31;         // thread’s lane id
    const int warp_id = threadIdx.x >> 5;         // warp id inside block

    float warp_sum = warp_reduce_sum(local_sum);

    // Shared memory to collect one partial per warp
    extern __shared__ float sdata[];
    if (lane == 0) sdata[warp_id] = warp_sum;
    __syncthreads();

    // Final reduction by first warp
    float block_sum = 0.f;
    if (warp_id == 0) {
        block_sum = (lane < (blockDim.x >> 5)) ? sdata[lane] : 0.f;
        block_sum = warp_reduce_sum(block_sum);

        if (lane == 0) {
            float mean_val = block_sum / static_cast<float>(pooled_sz);
            out[nc] = tanhf(mean_val);
        }
    }
}

////////////////////////////////////////////////////////////////////////////////
// Host wrapper exposed to Python
////////////////////////////////////////////////////////////////////////////////
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(
        torch::Tensor input,
        float hard_min,
        float hard_max) {

    TORCH_CHECK(input.is_cuda(), "Input tensor must be on CUDA device");
    TORCH_CHECK(input.scalar_type() == at::kFloat,
                "Only float32 tensors are supported");
    TORCH_CHECK(input.dim() == 4, "Input must be a 4-D tensor [N,C,H,W]");
    TORCH_CHECK((input.size(2) & 1) == 0 && (input.size(3) & 1) == 0,
                "H and W must be even for 2×2 pooling");

    const int N = input.size(0);
    const int C = input.size(1);
    const int H = input.size(2);
    const int W = input.size(3);

    auto out = torch::empty({N, C, 1, 1}, input.options());

    // Tunable launch parameters
    constexpr int threads = 256;                  // 8 warps / block
    const int blocks      = N * C;
    const int sh_mem      = (threads >> 5) * sizeof(float); // one float / warp

    fused_pool_hardtanh_mean_tanh_kernel<<<blocks, threads, sh_mem>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W,
        hard_min,
        hard_max);

    return out;
}
"""

# ----------------------------------------------------------------------
# C++ prototypes exposed to Python
# ----------------------------------------------------------------------
cpp_src = """
torch::Tensor fused_pool_hardtanh_mean_tanh_cuda(torch::Tensor input,
                                                 float hard_min,
                                                 float hard_max);
"""

# ----------------------------------------------------------------------
# Build / load the extension
# ----------------------------------------------------------------------
fused_ops = load_inline(
    name="fused_pool_hardtanh_mean_tanh",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_pool_hardtanh_mean_tanh_cuda"],
    verbose=False,
)

# ----------------------------------------------------------------------
# PyTorch module using the improved fused kernel
# ----------------------------------------------------------------------
class ModelNew(nn.Module):
    """
    Model with ConvTranspose2d followed by a highly-optimised fused
    (MaxPool2d → HardTanh → Mean → Tanh) CUDA kernel.
    The Python interface mirrors the original architecture.
    """
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size,
                 stride,
                 padding,
                 maxpool_kernel_size,
                 maxpool_stride,
                 hardtanh_min,
                 hardtanh_max):
        super().__init__()

        self.conv_transpose = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
        )

        self.hardtanh_min = float(hardtanh_min)
        self.hardtanh_max = float(hardtanh_max)

    def forward(self, x):
        # 1. Standard ConvTranspose2d
        x = self.conv_transpose(x)

        # 2-5. Fused CUDA kernel
        x = fused_ops.fused_pool_hardtanh_mean_tanh_cuda(
            x,
            self.hardtanh_min,
            self.hardtanh_max,
        )
        return x
```

GOAL
────
- Improve latency and throughput on the target GPU.
- Maintain correctness within atol=1e-4 or rtol=1e-4.
- Preserve the public Python API (same inputs/outputs, shapes, dtypes).


OUTPUT RULES (STRICT) ────────────────────────────────────────────────
1. Inside the block, follow **exactly** this order:
   1. Imports – `torch`, `torch.nn`, `load_inline`.
   2. `source` – triple‑quoted CUDA string(s) (kernel + host wrapper).
   3. `cpp_src` – prototypes for *all* kernels you expose.
   4. **One** `load_inline` call per kernel group.
   5. `class ModelNew(nn.Module)` – mirrors original inputs/outputs but calls
      your CUDA kernels.
2. **Do NOT include** testing code, `if __name__ == "__main__"`, or extra prose.

```python
# <your corrected code>
```
# ==========================================================
